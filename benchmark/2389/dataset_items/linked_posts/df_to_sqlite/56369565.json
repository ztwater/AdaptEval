{
    "items": [
        {
            "tags": [
                "python",
                "sql",
                "pandas"
            ],
            "comments": [
                {
                    "owner": {
                        "account_id": 3948330,
                        "reputation": 5967,
                        "user_id": 3259896,
                        "user_type": "registered",
                        "accept_rate": 60,
                        "display_name": "SantoshGupta7"
                    },
                    "reply_to_user": {
                        "account_id": 1523410,
                        "reputation": 106023,
                        "user_id": 1422451,
                        "user_type": "registered",
                        "accept_rate": 78,
                        "display_name": "Parfait"
                    },
                    "edited": false,
                    "score": 0,
                    "creation_date": 1559431503,
                    "post_id": 56369565,
                    "comment_id": 99419368,
                    "link": "https://stackoverflow.com/questions/56369565/large-6-million-rows-pandas-df-causes-memory-error-with-to-sql-when-chunksi#comment99419368_56369565",
                    "body": "the problem with that is to_csv takes many hours; it won&#39;t be a sustainable practice for me."
                }
            ],
            "answers": [
                {
                    "comments": [
                        {
                            "owner": {
                                "account_id": 3948330,
                                "reputation": 5967,
                                "user_id": 3259896,
                                "user_type": "registered",
                                "accept_rate": 60,
                                "display_name": "SantoshGupta7"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1559411696,
                            "post_id": 56369713,
                            "comment_id": 99415439,
                            "link": "https://stackoverflow.com/questions/56369565/large-6-million-rows-pandas-df-causes-memory-error-with-to-sql-when-chunksi/56369713#comment99415439_56369713",
                            "body": "I updated the post with what came out after cntrl c for two rungs, one at the beginning and one right before crash. It&#39;s going into sqlalchemy , so perhaps the bug is there?"
                        },
                        {
                            "owner": {
                                "account_id": 3948330,
                                "reputation": 5967,
                                "user_id": 3259896,
                                "user_type": "registered",
                                "accept_rate": 60,
                                "display_name": "SantoshGupta7"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1559412152,
                            "post_id": 56369713,
                            "comment_id": 99415562,
                            "link": "https://stackoverflow.com/questions/56369565/large-6-million-rows-pandas-df-causes-memory-error-with-to-sql-when-chunksi/56369713#comment99415562_56369713",
                            "body": "I did one more run right before it crashed (tried to catch it before that as an intermediate) it stopped at where you said it would. Not sure what this means."
                        },
                        {
                            "owner": {
                                "account_id": 515061,
                                "reputation": 369119,
                                "user_id": 1240268,
                                "user_type": "registered",
                                "accept_rate": 90,
                                "display_name": "Andy Hayden"
                            },
                            "reply_to_user": {
                                "account_id": 3948330,
                                "reputation": 5967,
                                "user_id": 3259896,
                                "user_type": "registered",
                                "accept_rate": 60,
                                "display_name": "SantoshGupta7"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1559418002,
                            "post_id": 56369713,
                            "comment_id": 99416878,
                            "link": "https://stackoverflow.com/questions/56369565/large-6-million-rows-pandas-df-causes-memory-error-with-to-sql-when-chunksi/56369713#comment99416878_56369713",
                            "body": "@SantoshGupta7 have you tried in an explicit loop (rather than using chunk), ie. <code>for i in range(100): df.iloc[i * 100000:(i+1):100000].to_sql(...)</code> ?"
                        },
                        {
                            "owner": {
                                "account_id": 3948330,
                                "reputation": 5967,
                                "user_id": 3259896,
                                "user_type": "registered",
                                "accept_rate": 60,
                                "display_name": "SantoshGupta7"
                            },
                            "edited": false,
                            "score": 1,
                            "creation_date": 1559419780,
                            "post_id": 56369713,
                            "comment_id": 99417247,
                            "link": "https://stackoverflow.com/questions/56369565/large-6-million-rows-pandas-df-causes-memory-error-with-to-sql-when-chunksi/56369713#comment99417247_56369713",
                            "body": "I&#39;ll try this next. It looks like if I set if_exists = append in to_sql, it&#39;ll just add the new values. I&#39;ll try this soon"
                        },
                        {
                            "owner": {
                                "account_id": 3948330,
                                "reputation": 5967,
                                "user_id": 3259896,
                                "user_type": "registered",
                                "accept_rate": 60,
                                "display_name": "SantoshGupta7"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1559425159,
                            "post_id": 56369713,
                            "comment_id": 99418295,
                            "link": "https://stackoverflow.com/questions/56369565/large-6-million-rows-pandas-df-causes-memory-error-with-to-sql-when-chunksi/56369713#comment99418295_56369713",
                            "body": "@Andry Hayden I have tried this, but it still crashes. It eventually eats up all the RAM, and around halfway through, there is a crash. But now I think it&#39;s a little more clear that the issue is with the database and not pandas to_sql. It looks like it&#39;s continuing to hold all the data in active memory?"
                        },
                        {
                            "owner": {
                                "account_id": 515061,
                                "reputation": 369119,
                                "user_id": 1240268,
                                "user_type": "registered",
                                "accept_rate": 90,
                                "display_name": "Andy Hayden"
                            },
                            "reply_to_user": {
                                "account_id": 3948330,
                                "reputation": 5967,
                                "user_id": 3259896,
                                "user_type": "registered",
                                "accept_rate": 60,
                                "display_name": "SantoshGupta7"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1559433567,
                            "post_id": 56369713,
                            "comment_id": 99419641,
                            "link": "https://stackoverflow.com/questions/56369565/large-6-million-rows-pandas-df-causes-memory-error-with-to-sql-when-chunksi/56369713#comment99419641_56369713",
                            "body": "@SantoshGupta7 perhaps try a raw connection, rather than using SQLEngine?\\"
                        },
                        {
                            "owner": {
                                "account_id": 3948330,
                                "reputation": 5967,
                                "user_id": 3259896,
                                "user_type": "registered",
                                "accept_rate": 60,
                                "display_name": "SantoshGupta7"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1559440683,
                            "post_id": 56369713,
                            "comment_id": 99420357,
                            "link": "https://stackoverflow.com/questions/56369565/large-6-million-rows-pandas-df-causes-memory-error-with-to-sql-when-chunksi/56369713#comment99420357_56369713",
                            "body": "I&#39;m not too familiar with this, but from my initial search, this would mean using sqlite3 instead of sqlalchemy?"
                        },
                        {
                            "owner": {
                                "account_id": 515061,
                                "reputation": 369119,
                                "user_id": 1240268,
                                "user_type": "registered",
                                "accept_rate": 90,
                                "display_name": "Andy Hayden"
                            },
                            "reply_to_user": {
                                "account_id": 3948330,
                                "reputation": 5967,
                                "user_id": 3259896,
                                "user_type": "registered",
                                "accept_rate": 60,
                                "display_name": "SantoshGupta7"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1559445248,
                            "post_id": 56369713,
                            "comment_id": 99420752,
                            "link": "https://stackoverflow.com/questions/56369565/large-6-million-rows-pandas-df-causes-memory-error-with-to-sql-when-chunksi/56369713#comment99420752_56369713",
                            "body": "Yes, just an idea"
                        },
                        {
                            "owner": {
                                "account_id": 3948330,
                                "reputation": 5967,
                                "user_id": 3259896,
                                "user_type": "registered",
                                "accept_rate": 60,
                                "display_name": "SantoshGupta7"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1559508368,
                            "post_id": 56369713,
                            "comment_id": 99433292,
                            "link": "https://stackoverflow.com/questions/56369565/large-6-million-rows-pandas-df-causes-memory-error-with-to-sql-when-chunksi/56369713#comment99433292_56369713",
                            "body": "I was able to insert all the data by just continuing where I left off after the memory crash with the <code>for i in range(100): df.iloc[i * 100000:(i+1):100000].to_sql(...)</code> method. But I will do direct insertions next time. I started a new SO question to look into sqlalchemy <a href=\"https://stackoverflow.com/questions/56418825/repeated-insertions-into-sqlite-database-via-sqlalchemy-causing-memory-leak\" title=\"repeated insertions into sqlite database via sqlalchemy causing memory leak\">stackoverflow.com/questions/56418825/&hellip;</a> ."
                        }
                    ],
                    "owner": {
                        "account_id": 515061,
                        "reputation": 369119,
                        "user_id": 1240268,
                        "user_type": "registered",
                        "accept_rate": 90,
                        "display_name": "Andy Hayden"
                    },
                    "comment_count": 9,
                    "is_accepted": true,
                    "score": 3,
                    "last_activity_date": 1559511501,
                    "last_edit_date": 1559511501,
                    "creation_date": 1559172091,
                    "answer_id": 56369713,
                    "question_id": 56369565,
                    "link": "https://stackoverflow.com/questions/56369565/large-6-million-rows-pandas-df-causes-memory-error-with-to-sql-when-chunksi/56369713#56369713",
                    "body": "<p>From stepping through the code I <em>think</em> it's <a href=\"https://github.com/pandas-dev/pandas/blob/a91da0c94e541217865cdf52b9f6ea694f0493d3/pandas/io/sql.py#L676\" rel=\"nofollow noreferrer\">this line</a>, which reads creates a bunch of DataFrames:</p>\n\n<pre><code>chunk_iter = zip(*[arr[start_i:end_i] for arr in data_list])\n</code></pre>\n\n<p>Which looks like it's <em>probably</em> a bug. Specifically this happens prior to database insertion, in preparation.</p>\n\n<p><em>One trick you can do is CTRL-C whilst the memory is rapidly increasing, and see which line is paused on (my bet is this one).</em></p>\n\n<p>User Edit:</p>\n\n<p>Problem was solved by using</p>\n\n<p><code>explicit loop (rather than using chunk), ie. for i in range(100): df.iloc[i * 100000:(i+1):100000].to_sql(...)</code></p>\n\n<p>Which still resulted in memory errors, but allowed the user to continue where loop left off before the crash. </p>\n\n<p>A more robust solution is to \"perhaps try a raw connection, rather than using SQLEngine?\\ \"\nBut user didn't have a chance to try this </p>\n"
                },
                {
                    "comments": [
                        {
                            "owner": {
                                "account_id": 11223740,
                                "reputation": 279,
                                "user_id": 8234087,
                                "user_type": "registered",
                                "display_name": "DaveP"
                            },
                            "edited": false,
                            "score": 2,
                            "creation_date": 1620323538,
                            "post_id": 65836405,
                            "comment_id": 119174035,
                            "link": "https://stackoverflow.com/questions/56369565/large-6-million-rows-pandas-df-causes-memory-error-with-to-sql-when-chunksi/65836405#comment119174035_65836405",
                            "body": "This answer was helpful for me, from the attached link, connection.execution_options(stream_results=True).                      solved it for me."
                        }
                    ],
                    "owner": {
                        "account_id": 10081165,
                        "reputation": 289,
                        "user_id": 7451414,
                        "user_type": "registered",
                        "display_name": "juanjosegdoj"
                    },
                    "comment_count": 1,
                    "is_accepted": false,
                    "score": 5,
                    "last_activity_date": 1611266627,
                    "creation_date": 1611266627,
                    "answer_id": 65836405,
                    "question_id": 56369565,
                    "link": "https://stackoverflow.com/questions/56369565/large-6-million-rows-pandas-df-causes-memory-error-with-to-sql-when-chunksi/65836405#65836405",
                    "body": "<p>I've used <strong>df.to_sql</strong> for 1 year and now I'm struggling with the fact I running big resources and it wasn't working. I realized that chucksize overload your memory, pandas loaded in memory and after that sent it by chuncks. I had to control directly using sql. (here is where I found the solution -&gt; <a href=\"https://github.com/pandas-dev/pandas/issues/12265\" rel=\"noreferrer\">https://github.com/pandas-dev/pandas/issues/12265</a> I really encourage you to read untill the end.)</p>\n<p>If you need to read data from your databaset without overloading your memory, check this piece of code:</p>\n<pre><code>def get_data_by_chunks(cls, table, chunksize: int) -&gt; iter:\n    with MysqlClient.get_engine().begin() as conn:\n        query_count = &quot;select COUNT(*) from my_query&quot;\n        row_count = conn.execute(query_count, where).fetchone()[0]\n\n        for i in range(math.ceil(row_count / chunksize)):\n            query = &quot;&quot;&quot;\n               SELECT * FROM my_table\n               WHERE my_filiters\n               LIMIT {offset}, {row_count};\n             &quot;&quot;&quot;\n            yield pd.read_sql(query, conn)\n\nfor df in get_data_by_chunks(cls, table, chunksize: int):\n    print(df.shape)\n</code></pre>\n"
                },
                {
                    "owner": {
                        "account_id": 80571,
                        "reputation": 11497,
                        "user_id": 227755,
                        "user_type": "registered",
                        "accept_rate": 90,
                        "display_name": "nurettin"
                    },
                    "comment_count": 0,
                    "is_accepted": false,
                    "score": 1,
                    "last_activity_date": 1640546736,
                    "last_edit_date": 1640546736,
                    "creation_date": 1640541962,
                    "answer_id": 70488765,
                    "question_id": 56369565,
                    "link": "https://stackoverflow.com/questions/56369565/large-6-million-rows-pandas-df-causes-memory-error-with-to-sql-when-chunksi/70488765#70488765",
                    "body": "<p>I know this isn't an answer to the question. I'm writing this for people who are in a hurry and to_sql runs out of memory quickly for them. I read <a href=\"https://github.com/pandas-dev/pandas/blob/a769e389343dec0d59df3022a5f0b503ffd42dfc/pandas/io/sql.py#L2146\" rel=\"nofollow noreferrer\">the source</a> and came up with my own iterative pandas-to-sqlite function which uses existing APIs and doesn't copy the dataframe. It currently uses SQLiteDatabase which comes with a warning because this code is incompatible with sqlalchemy.</p>\n<p>This one is tested on a 3 column 94 million lines (2.2GB) indexed dataframe. On my rather old machine it takes less than 5 minutes and less than 5GB RAM to insert the entire data. I also added tqdm for a good measure.</p>\n<pre><code>import pandas as pd\nfrom pandas.io.sql import SQLiteDatabase, SQLiteTable\nimport sqlite3\nfrom tqdm import tqdm\n\ndef df_to_sqlite(df: pd.DataFrame, db_file_name: str, table_name: str, chunk_size = 1_000_000):\n    # see https://stackoverflow.com/a/70488765/227755\n    con = sqlite3.connect(db_file_name)\n    db = SQLiteDatabase(con=con)\n    table = SQLiteTable(table_name, db, df, index=True, if_exists=&quot;fail&quot;, dtype=None)\n    table.create()  # can be optimized further by postponing index creation, but that means we use private/protected APIs.\n    insert = table.insert_statement(num_rows=1)  # single insert statement\n    it = df.itertuples(index=True, name=None)  # just regular tuples\n    pb = tqdm(it, total=len(df))  # not needed but nice to have\n    with con:\n        while True:\n            con.execute(&quot;begin&quot;)\n            try:\n                for c in range(0, chunk_size):\n                    row = next(it, None)\n                    if row is None:\n                        pb.update(c)\n                        return\n                    con.execute(insert, row)\n                pb.update(chunk_size)\n            finally:\n                con.execute(&quot;commit&quot;)\n</code></pre>\n"
                }
            ],
            "owner": {
                "account_id": 3948330,
                "reputation": 5967,
                "user_id": 3259896,
                "user_type": "registered",
                "accept_rate": 60,
                "display_name": "SantoshGupta7"
            },
            "comment_count": 1,
            "is_answered": true,
            "accepted_answer_id": 56369713,
            "answer_count": 3,
            "score": 6,
            "last_activity_date": 1640546736,
            "creation_date": 1559170753,
            "last_edit_date": 1559429140,
            "question_id": 56369565,
            "link": "https://stackoverflow.com/questions/56369565/large-6-million-rows-pandas-df-causes-memory-error-with-to-sql-when-chunksi",
            "title": "Large (6 million rows) pandas df causes memory error with `to_sql ` when chunksize =100, but can easily save file of 100,000 with no chunksize",
            "body": "<p>I created a large database in Pandas, about 6 million rows of text data. I wanted to save this as a SQL database file, but when I try to save it, I get an out of memory RAM error. I even reduced the chuck size to 100 and it still crashes. </p>\n\n<p>However, if I just have smaller version of that dataframe with 100,000 rows, and save it to a database with no chucksize specified, I have no issues saving tha dataframe. </p>\n\n<p>This is my code </p>\n\n<pre><code>from sqlalchemy import create_engine\nengine = sqlalchemy.create_engine(\"sqlite:///databasefile.db\")\ndataframe.to_sql(\"CS_table\", engine, chunksize = 100)\n</code></pre>\n\n<p>My understanding was that since it's only processing 100 rows at a time, the RAM usage should reflect that of a save of 100 rows. Is there something else happening behind the scenes? Perhaps multi-threading?</p>\n\n<p>Before I run this code, I am using 4.8 GB RAM, out of the 12.8 GB RAM available in Google Colab. Running the above code eats up all the RAM until the enviroment crashes. </p>\n\n<p>I would like to be able to save my pandas dataframe to a SQL file without my environment crashing. The environment I am in is Google Colab. The pandas datafame is 2 columns, ~6 million rows. Each cell contains about this much text:</p>\n\n<blockquote>\n  <p>\"The dominant sequence transduction models are based on complex\n  recurrent or convolutional neural networks in an encoder-decoder\n  configuration. The best performing models also connect the encoder and\n  decoder through an attention mechanism. We propose a new simple\n  network architecture, the Transformer, based solely on attention\n  mechanisms, dispensing with recurrence and convolutions entirely.\n  Experiments on two machine translation tasks show these models to be\n  superior in quality while being more parallelizable and requiring\n  significantly less time to train. Our model achieves 28.4 BLEU on the\n  WMT 2014 English-to-German translation task, improving over the\n  existing best results, including ensembles by over 2 BLEU. On the WMT\n  2014 English-to-French translation task, our model establishes a new\n  single-model state-of-the-art BLEU score of 41.8 after training for\n  3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer\n  generalizes well to other tasks by applying it successfully to English\n  constituency parsing both with large and limited training data.\"</p>\n</blockquote>\n\n<p>Edit:</p>\n\n<p>I did a keyboard interrupts at various stages. Here is the results of a keyboard interrupt after the first jump in RAM</p>\n\n<pre><code>---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\n&lt;ipython-input-22-51b6e444f80d&gt; in &lt;module&gt;()\n----&gt; 1 dfAllT.to_sql(\"CS_table23\", engine, chunksize = 100)\n\n12 frames\n/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py in to_sql(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\n   2529         sql.to_sql(self, name, con, schema=schema, if_exists=if_exists,\n   2530                    index=index, index_label=index_label, chunksize=chunksize,\n-&gt; 2531                    dtype=dtype, method=method)\n   2532 \n   2533     def to_pickle(self, path, compression='infer',\n\n/usr/local/lib/python3.6/dist-packages/pandas/io/sql.py in to_sql(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\n    458     pandas_sql.to_sql(frame, name, if_exists=if_exists, index=index,\n    459                       index_label=index_label, schema=schema,\n--&gt; 460                       chunksize=chunksize, dtype=dtype, method=method)\n    461 \n    462 \n\n/usr/local/lib/python3.6/dist-packages/pandas/io/sql.py in to_sql(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method)\n   1172                          schema=schema, dtype=dtype)\n   1173         table.create()\n-&gt; 1174         table.insert(chunksize, method=method)\n   1175         if (not name.isdigit() and not name.islower()):\n   1176             # check for potentially case sensitivity issues (GH7815)\n\n/usr/local/lib/python3.6/dist-packages/pandas/io/sql.py in insert(self, chunksize, method)\n    684 \n    685                 chunk_iter = zip(*[arr[start_i:end_i] for arr in data_list])\n--&gt; 686                 exec_insert(conn, keys, chunk_iter)\n    687 \n    688     def _query_iterator(self, result, chunksize, columns, coerce_float=True,\n\n/usr/local/lib/python3.6/dist-packages/pandas/io/sql.py in _execute_insert(self, conn, keys, data_iter)\n    597         \"\"\"\n    598         data = [dict(zip(keys, row)) for row in data_iter]\n--&gt; 599         conn.execute(self.table.insert(), data)\n    600 \n    601     def _execute_insert_multi(self, conn, keys, data_iter):\n\n/usr/local/lib/python3.6/dist-packages/sqlalchemy/engine/base.py in execute(self, object_, *multiparams, **params)\n    986             raise exc.ObjectNotExecutableError(object_)\n    987         else:\n--&gt; 988             return meth(self, multiparams, params)\n    989 \n    990     def _execute_function(self, func, multiparams, params):\n\n/usr/local/lib/python3.6/dist-packages/sqlalchemy/sql/elements.py in _execute_on_connection(self, connection, multiparams, params)\n    285     def _execute_on_connection(self, connection, multiparams, params):\n    286         if self.supports_execution:\n--&gt; 287             return connection._execute_clauseelement(self, multiparams, params)\n    288         else:\n    289             raise exc.ObjectNotExecutableError(self)\n\n/usr/local/lib/python3.6/dist-packages/sqlalchemy/engine/base.py in _execute_clauseelement(self, elem, multiparams, params)\n   1105             distilled_params,\n   1106             compiled_sql,\n-&gt; 1107             distilled_params,\n   1108         )\n   1109         if self._has_events or self.engine._has_events:\n\n/usr/local/lib/python3.6/dist-packages/sqlalchemy/engine/base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)\n   1246         except BaseException as e:\n   1247             self._handle_dbapi_exception(\n-&gt; 1248                 e, statement, parameters, cursor, context\n   1249             )\n   1250 \n\n/usr/local/lib/python3.6/dist-packages/sqlalchemy/engine/base.py in _handle_dbapi_exception(self, e, statement, parameters, cursor, context)\n   1466                 util.raise_from_cause(sqlalchemy_exception, exc_info)\n   1467             else:\n-&gt; 1468                 util.reraise(*exc_info)\n   1469 \n   1470         finally:\n\n/usr/local/lib/python3.6/dist-packages/sqlalchemy/util/compat.py in reraise(tp, value, tb, cause)\n    127         if value.__traceback__ is not tb:\n    128             raise value.with_traceback(tb)\n--&gt; 129         raise value\n    130 \n    131     def u(s):\n\n/usr/local/lib/python3.6/dist-packages/sqlalchemy/engine/base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)\n   1222                 if not evt_handled:\n   1223                     self.dialect.do_executemany(\n-&gt; 1224                         cursor, statement, parameters, context\n   1225                     )\n   1226             elif not parameters and context.no_parameters:\n\n/usr/local/lib/python3.6/dist-packages/sqlalchemy/engine/default.py in do_executemany(self, cursor, statement, parameters, context)\n    545 \n    546     def do_executemany(self, cursor, statement, parameters, context=None):\n--&gt; 547         cursor.executemany(statement, parameters)\n    548 \n    549     def do_execute(self, cursor, statement, parameters, context=None):\n\nKeyboardInterrupt: \n</code></pre>\n\n<p>Here is the result if I do a keyboard interrupt just before it crashes</p>\n\n<pre><code>ERROR:root:Internal Python error in the inspect module.\nBelow is the traceback from this internal error.\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-24-68b60fe221fe&gt;\", line 1, in &lt;module&gt;\n    dfAllT.to_sql(\"CS_table22\", engine, chunksize = 100)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\", line 2531, in to_sql\n    dtype=dtype, method=method)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/sql.py\", line 460, in to_sql\n    chunksize=chunksize, dtype=dtype, method=method)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/sql.py\", line 1174, in to_sql\n    table.insert(chunksize, method=method)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/sql.py\", line 686, in insert\n    exec_insert(conn, keys, chunk_iter)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/sql.py\", line 599, in _execute_insert\n    conn.execute(self.table.insert(), data)\n  File \"/usr/local/lib/python3.6/dist-packages/sqlalchemy/engine/base.py\", line 988, in execute\n    return meth(self, multiparams, params)\n  File \"/usr/local/lib/python3.6/dist-packages/sqlalchemy/sql/elements.py\", line 287, in _execute_on_connection\n    return connection._execute_clauseelement(self, multiparams, params)\n  File \"/usr/local/lib/python3.6/dist-packages/sqlalchemy/engine/base.py\", line 1107, in _execute_clauseelement\n    distilled_params,\n  File \"/usr/local/lib/python3.6/dist-packages/sqlalchemy/engine/base.py\", line 1248, in _execute_context\n    e, statement, parameters, cursor, context\n  File \"/usr/local/lib/python3.6/dist-packages/sqlalchemy/engine/base.py\", line 1468, in _handle_dbapi_exception\n    util.reraise(*exc_info)\n  File \"/usr/local/lib/python3.6/dist-packages/sqlalchemy/util/compat.py\", line 129, in reraise\n    raise value\n  File \"/usr/local/lib/python3.6/dist-packages/sqlalchemy/engine/base.py\", line 1224, in _execute_context\n    cursor, statement, parameters, context\n  File \"/usr/local/lib/python3.6/dist-packages/sqlalchemy/engine/default.py\", line 547, in do_executemany\n    cursor.executemany(statement, parameters)\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n    stb = value._render_traceback_()\nAttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n  File \"/usr/lib/python3.6/inspect.py\", line 1488, in getinnerframes\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n  File \"/usr/lib/python3.6/inspect.py\", line 1446, in getframeinfo\n    filename = getsourcefile(frame) or getfile(frame)\n  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n  File \"/usr/lib/python3.6/inspect.py\", line 739, in getmodule\n    f = getabsfile(module)\n  File \"/usr/lib/python3.6/inspect.py\", line 708, in getabsfile\n    _filename = getsourcefile(object) or getfile(object)\n  File \"/usr/lib/python3.6/inspect.py\", line 693, in getsourcefile\n    if os.path.exists(filename):\n  File \"/usr/lib/python3.6/genericpath.py\", line 19, in exists\n    os.stat(path)\nKeyboardInterrupt\n</code></pre>\n\n<p>I did another run right before it crashed, and this seemed to give another different result</p>\n\n<pre><code>ERROR:root:Internal Python error in the inspect module.\nBelow is the traceback from this internal error.\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-28-f18004debe33&gt;\", line 1, in &lt;module&gt;\n    dfAllT.to_sql(\"CS_table25\", engine, chunksize = 100)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\", line 2531, in to_sql\n    dtype=dtype, method=method)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/sql.py\", line 460, in to_sql\n    chunksize=chunksize, dtype=dtype, method=method)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/sql.py\", line 1174, in to_sql\n    table.insert(chunksize, method=method)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/sql.py\", line 686, in insert\n    exec_insert(conn, keys, chunk_iter)\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/sql.py\", line 598, in _execute_insert\n    data = [dict(zip(keys, row)) for row in data_iter]\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/io/sql.py\", line 598, in &lt;listcomp&gt;\n    data = [dict(zip(keys, row)) for row in data_iter]\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n    stb = value._render_traceback_()\nAttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n  File \"/usr/lib/python3.6/inspect.py\", line 1488, in getinnerframes\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n  File \"/usr/lib/python3.6/inspect.py\", line 1446, in getframeinfo\n    filename = getsourcefile(frame) or getfile(frame)\n  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n  File \"/usr/lib/python3.6/inspect.py\", line 742, in getmodule\n    os.path.realpath(f)] = module.__name__\n  File \"/usr/lib/python3.6/posixpath.py\", line 388, in realpath\n    path, ok = _joinrealpath(filename[:0], filename, {})\n  File \"/usr/lib/python3.6/posixpath.py\", line 421, in _joinrealpath\n    newpath = join(path, name)\nKeyboardInterrupt\n---------------------------------------------------------------------------\n</code></pre>\n\n<p>Other things I have tried:</p>\n\n<p>Using dropna to drop all none/nan values</p>\n\n<p>dfAllT = dfAllT.applymap(str) to make sure all my values are strings</p>\n\n<p>dfAllT.reset_index(drop=True, inplace=True) to make sure index is not out of alignment. </p>\n\n<p>Edit:</p>\n\n<p>Like what is mentioned in the comment, I have now tried to use to_sql in a loop. </p>\n\n<pre><code>for i in range(586147):\n    print(i)\n    dfAllT.iloc[i*10000:(i+1)*10000].to_sql('CS_table', engine, if_exists= 'append')\n</code></pre>\n\n<p>This operation eventually eats at my RAM, and eventually causes a crash about halfway through. I wonder if this suggests that sqlite is holding everything in memory, and if there is a work around.  </p>\n\n<p>Edit:</p>\n\n<p>I tried a few more things, shorter chucks, disposing the engine after each step and creating a new one. Still eventually ate of all RAM and crashed.  </p>\n\n<pre><code>for i in range(586147):\n    print(i)\n    engine = sqlalchemy.create_engine(\"sqlite:///CSTitlesSummariesData.db\")\n    dfAllT.iloc[i*10:(i+1)*10].to_sql('CS_table', engine, index = False, if_exists= 'append')\n    engine.dispose() \n    gc.collect \n</code></pre>\n\n<p>My thoughts:</p>\n\n<p>So it looks like somehow the entire database is somehow kept in active memory somehow. </p>\n\n<p>The pandas dataframe from which this is made from was 5 gigs (or at least that's how much RAM is being before I try to convert it to sqlite). My system crashes at around 12.72 gigs. I would imagine the sqlite database takes of less RAM than the pandas dataframe. </p>\n"
        }
    ],
    "has_more": false,
    "quota_max": 10000,
    "quota_remaining": 5371
}