{
    "items": [
        {
            "tags": [
                "optimization",
                "machine-learning",
                "statistics",
                "normal-distribution",
                "gradient-descent"
            ],
            "comments": [
                {
                    "owner": {
                        "account_id": 2680523,
                        "reputation": 562,
                        "user_id": 2315997,
                        "user_type": "registered",
                        "accept_rate": 100,
                        "display_name": "jschabs"
                    },
                    "edited": false,
                    "score": 0,
                    "creation_date": 1460646789,
                    "post_id": 34663539,
                    "comment_id": 60850405,
                    "link": "https://stackoverflow.com/questions/34663539/scipy-optimize-fmin-l-bfgs-b-returns-abnormal-termination-in-lnsrch#comment60850405_34663539",
                    "body": "I&#39;m facing similar problems. They all seem to be centered on the gradient function that I give the the optimizer. Do you know with 100% certainty that your gradient is completely correct?"
                },
                {
                    "owner": {
                        "account_id": 2266813,
                        "reputation": 958,
                        "user_id": 1995261,
                        "user_type": "registered",
                        "accept_rate": 100,
                        "display_name": "muammar"
                    },
                    "edited": false,
                    "score": 0,
                    "creation_date": 1529258561,
                    "post_id": 34663539,
                    "comment_id": 88798083,
                    "link": "https://stackoverflow.com/questions/34663539/scipy-optimize-fmin-l-bfgs-b-returns-abnormal-termination-in-lnsrch#comment88798083_34663539",
                    "body": "I am getting similar issues with L-BFGS when trying to maximize the log-likelihood of a function. I have to add that I am not passing the gradient of the function, but instead, I let L-BFGS approximate it. Sometimes I&#39;ve able to overcome the issue by using the Nelder\u2013Mead optimizer instead... Have you been able to solve this problem?"
                },
                {
                    "owner": {
                        "account_id": 5726283,
                        "reputation": 2522,
                        "user_id": 7250309,
                        "user_type": "registered",
                        "accept_rate": 83,
                        "display_name": "ap21"
                    },
                    "reply_to_user": {
                        "account_id": 2266813,
                        "reputation": 958,
                        "user_id": 1995261,
                        "user_type": "registered",
                        "accept_rate": 100,
                        "display_name": "muammar"
                    },
                    "edited": false,
                    "score": 0,
                    "creation_date": 1536720423,
                    "post_id": 34663539,
                    "comment_id": 91521496,
                    "link": "https://stackoverflow.com/questions/34663539/scipy-optimize-fmin-l-bfgs-b-returns-abnormal-termination-in-lnsrch#comment91521496_34663539",
                    "body": "@muammar, in my experience of using L-BFGS, it only works well if you provide an explicit derivative function. Otherwise, it gets lost quite easily."
                }
            ],
            "answers": [
                {
                    "comments": [
                        {
                            "owner": {
                                "account_id": 324108,
                                "reputation": 336,
                                "user_id": 644506,
                                "user_type": "registered",
                                "display_name": "Ilya Kolpakov"
                            },
                            "edited": false,
                            "score": 5,
                            "creation_date": 1541102088,
                            "post_id": 39155976,
                            "comment_id": 93112215,
                            "link": "https://stackoverflow.com/questions/34663539/scipy-optimize-fmin-l-bfgs-b-returns-abnormal-termination-in-lnsrch/39155976#comment93112215_39155976",
                            "body": "This error message also is printed if the target function (or possibly gradient) becomes <code>nan</code>."
                        },
                        {
                            "owner": {
                                "account_id": 2348943,
                                "reputation": 5526,
                                "user_id": 2058333,
                                "user_type": "registered",
                                "accept_rate": 46,
                                "display_name": "El Dude"
                            },
                            "edited": false,
                            "score": 1,
                            "creation_date": 1541182406,
                            "post_id": 39155976,
                            "comment_id": 93142340,
                            "link": "https://stackoverflow.com/questions/34663539/scipy-optimize-fmin-l-bfgs-b-returns-abnormal-termination-in-lnsrch/39155976#comment93142340_39155976",
                            "body": "&quot;The code tries something called line search a total of 20 times in the descent direction that you provide and realizes that you are NOT telling it to go downhill, but uphill.&quot; \u2013\u2013 Would that not mean that it has found a local optimum and should finish instead of throwing an error?"
                        },
                        {
                            "owner": {
                                "account_id": 1322292,
                                "reputation": 4212,
                                "user_id": 1267849,
                                "user_type": "registered",
                                "accept_rate": 87,
                                "display_name": "Wilmer E. Henao"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1542054324,
                            "post_id": 39155976,
                            "comment_id": 93422900,
                            "link": "https://stackoverflow.com/questions/34663539/scipy-optimize-fmin-l-bfgs-b-returns-abnormal-termination-in-lnsrch/39155976#comment93422900_39155976",
                            "body": "Nocedal&#39;s code catches convergence in another place in the code."
                        }
                    ],
                    "owner": {
                        "account_id": 1322292,
                        "reputation": 4212,
                        "user_id": 1267849,
                        "user_type": "registered",
                        "accept_rate": 87,
                        "display_name": "Wilmer E. Henao"
                    },
                    "comment_count": 3,
                    "is_accepted": true,
                    "score": 85,
                    "last_activity_date": 1472238050,
                    "last_edit_date": 1472238050,
                    "creation_date": 1472165786,
                    "answer_id": 39155976,
                    "question_id": 34663539,
                    "link": "https://stackoverflow.com/questions/34663539/scipy-optimize-fmin-l-bfgs-b-returns-abnormal-termination-in-lnsrch/39155976#39155976",
                    "body": "<p>Scipy calls the original L-BFGS-B implementation. Which is some fortran77 (old but beautiful and superfast code) and our problem is that the descent direction is actually going up.  The problem starts on line 2533 (link to the code at the bottom)</p>\n\n<pre><code>gd = ddot(n,g,1,d,1)\n  if (ifun .eq. 0) then\n     gdold=gd\n     if (gd .ge. zero) then\nc                               the directional derivative &gt;=0.\nc                               Line search is impossible.\n        if (iprint .ge. 0) then\n            write(0,*)' ascent direction in projection gd = ', gd\n        endif\n        info = -4\n        return\n     endif\n  endif\n</code></pre>\n\n<p>In other words, you are telling it to go down the hill by going up the hill. The code tries something called line search a total of 20 times in the descent direction that you provide and realizes that you are NOT telling it to go downhill, but uphill. All 20 times.</p>\n\n<p>The guy who wrote it (Jorge Nocedal, who by the way is a very smart guy) put 20 because pretty much that's enough. Machine epsilon is 10E-16, I think 20 is actually a little too much. So, my money for most people having this problem is that <strong>your gradient does not match your function</strong>.</p>\n\n<p>Now, it could also be that \"2. rounding errors dominate computation\". By this, he means that your function is a very flat surface in which increases are of the order of machine epsilon (in which case you could perhaps rescale the function),\nNow, I was thiking that maybe there should be a third option, when your function is too weird. Oscillations? I could see something like $\\sin({\\frac{1}{x}})$ causing this kind of problem. But I'm not a smart guy, so don't assume that there's a third case.</p>\n\n<p>So I think the OP's solution should be that your function is too flat. Or look at the fortran code.</p>\n\n<p><a href=\"https://github.com/scipy/scipy/blob/master/scipy/optimize/lbfgsb/lbfgsb.f\" rel=\"noreferrer\">https://github.com/scipy/scipy/blob/master/scipy/optimize/lbfgsb/lbfgsb.f</a></p>\n\n<p>Here's line search for those who want to see it. <a href=\"https://en.wikipedia.org/wiki/Line_search\" rel=\"noreferrer\">https://en.wikipedia.org/wiki/Line_search</a></p>\n\n<p>Note. This is 7 months too late. I put it here for future's sake.</p>\n"
                },
                {
                    "comments": [
                        {
                            "owner": {
                                "account_id": 2373127,
                                "reputation": 5312,
                                "user_id": 2077270,
                                "user_type": "registered",
                                "accept_rate": 67,
                                "display_name": "dermen"
                            },
                            "edited": false,
                            "score": 1,
                            "creation_date": 1575233646,
                            "post_id": 52793253,
                            "comment_id": 104487591,
                            "link": "https://stackoverflow.com/questions/34663539/scipy-optimize-fmin-l-bfgs-b-returns-abnormal-termination-in-lnsrch/52793253#comment104487591_52793253",
                            "body": "Reducing epsilon by 4 orders of magnitude helped in my case!"
                        },
                        {
                            "owner": {
                                "account_id": 15035730,
                                "reputation": 584,
                                "user_id": 10852215,
                                "user_type": "registered",
                                "display_name": "learningthemachine"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1581263745,
                            "post_id": 52793253,
                            "comment_id": 106364327,
                            "link": "https://stackoverflow.com/questions/34663539/scipy-optimize-fmin-l-bfgs-b-returns-abnormal-termination-in-lnsrch/52793253#comment106364327_52793253",
                            "body": "where does scipy call this function? I want to know where i can edit the params of the L-BFGS-B"
                        },
                        {
                            "owner": {
                                "account_id": 5539941,
                                "reputation": 1643,
                                "user_id": 4547005,
                                "user_type": "registered",
                                "accept_rate": 67,
                                "display_name": "toliveira"
                            },
                            "reply_to_user": {
                                "account_id": 15035730,
                                "reputation": 584,
                                "user_id": 10852215,
                                "user_type": "registered",
                                "display_name": "learningthemachine"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1582034974,
                            "post_id": 52793253,
                            "comment_id": 106631865,
                            "link": "https://stackoverflow.com/questions/34663539/scipy-optimize-fmin-l-bfgs-b-returns-abnormal-termination-in-lnsrch/52793253#comment106631865_52793253",
                            "body": "@learningthemachine, scipy.optimize.fmin_l_bfgs_b is probably called in many places. It is there so you can call it whenever you want."
                        }
                    ],
                    "owner": {
                        "account_id": 5539941,
                        "reputation": 1643,
                        "user_id": 4547005,
                        "user_type": "registered",
                        "accept_rate": 67,
                        "display_name": "toliveira"
                    },
                    "comment_count": 3,
                    "is_accepted": false,
                    "score": 8,
                    "last_activity_date": 1539436024,
                    "creation_date": 1539436024,
                    "answer_id": 52793253,
                    "question_id": 34663539,
                    "link": "https://stackoverflow.com/questions/34663539/scipy-optimize-fmin-l-bfgs-b-returns-abnormal-termination-in-lnsrch/52793253#52793253",
                    "body": "<p>As pointed out in the answer by Wilmer E. Henao, the problem is probably in the gradient. Since you are using <code>approx_grad=True</code>, the gradient is calculated numerically. In this case, reducing the value of <code>epsilon</code>, which is the step size used for numerically calculating the gradient, can help.</p>\n"
                },
                {
                    "owner": {
                        "account_id": 2448753,
                        "reputation": 2458,
                        "user_id": 2135504,
                        "user_type": "registered",
                        "display_name": "gebbissimo"
                    },
                    "comment_count": 0,
                    "is_accepted": false,
                    "score": 7,
                    "last_activity_date": 1549636347,
                    "last_edit_date": 1549636347,
                    "creation_date": 1549630075,
                    "answer_id": 54592829,
                    "question_id": 34663539,
                    "link": "https://stackoverflow.com/questions/34663539/scipy-optimize-fmin-l-bfgs-b-returns-abnormal-termination-in-lnsrch/54592829#54592829",
                    "body": "<p>I also got the error \"ABNORMAL_TERMINATION_IN_LNSRCH\" using the L-BFGS-B optimizer.</p>\n\n<p>While my gradient function pointed in the right direction, I rescaled the <em>actual</em> gradient of the function by its L2-norm. Removing that or adding another appropriate type of rescaling worked. Before, I guess that the gradient was so large that it went out of bounds immediately.</p>\n\n<p>The problem from OP was unbounded if I read correctly, so this will certainly not help in this problem setting. However, googling the error \"ABNORMAL_TERMINATION_IN_LNSRCH\" yields this page as one of the first results, so it might help others...</p>\n"
                },
                {
                    "owner": {
                        "account_id": 20481090,
                        "reputation": 51,
                        "user_id": 15029285,
                        "user_type": "registered",
                        "display_name": "kapytaine"
                    },
                    "comment_count": 0,
                    "is_accepted": false,
                    "score": 4,
                    "last_activity_date": 1649666808,
                    "last_edit_date": 1649666808,
                    "creation_date": 1649666326,
                    "answer_id": 71824901,
                    "question_id": 34663539,
                    "link": "https://stackoverflow.com/questions/34663539/scipy-optimize-fmin-l-bfgs-b-returns-abnormal-termination-in-lnsrch/71824901#71824901",
                    "body": "<p>I had a similar problem recently. I sometimes encounter the ABNORMAL_TERMINATION_IN_LNSRCH message after using <code>fmin_l_bfgs_b</code> function of scipy. I try to give additional explanations of the reason why I get this. I am looking for complementary details or corrections if I am wrong.</p>\n<p>In my case, I provide the gradient function, so <code>approx_grad=False</code>. <strong>My cost function and the gradient are consistent</strong>. I double-checked it and the optimization actually works most of the time. <strong>When I get ABNORMAL_TERMINATION_IN_LNSRCH, the solution is not optimal</strong>, not even close (even this is a subjective point of view). I can overcome this issue by modifying the <code>maxls</code> argument. <strong>Increasing <code>maxls</code> helps to solve this issue to finally get the optimal solution</strong>. <strong>However, I noted that sometimes a smaller <code>maxls</code>, than the one that produces ABNORMAL_TERMINATION_IN_LNSRCH, results in a converging solution</strong>. <a href=\"https://i.stack.imgur.com/lmbNF.png\" rel=\"nofollow noreferrer\">A dataframe summarizes the results</a>. I was surprised to observe this. I expected that reducing <code>maxls</code> would not improve the result. For this reason, I tried to read the paper describing the line search algorithm but I had trouble to understand it.</p>\n<p>The line &quot;<a href=\"https://www.ii.uib.no/%7Elennart/drgrad/More1994.pdf\" rel=\"nofollow noreferrer\">search algorithm</a> generates a sequence of\nnested intervals {I<sub>k</sub>} and a sequence of iterates \u03b1<sub>k</sub> \u2208 I<sub>k</sub> \u2229 [\u03b1<sub>min</sub> ; \u03b1<sub>max</sub>] according to the [...] procedure&quot;. If I understand well, I would say that the <code>maxls</code> argument specifies the length of this sequence. At the end of the <code>maxls</code> iterations (or less if the algorithm terminates in fewer iterations), the line search stops. A final trial point is generated within the final interval I<sub>maxls</sub>. I would say the the formula does not guarantee to get an \u03b1<sub>maxls</sub> that respects the two update conditions, the minimum decrease and the curvature, especially when the interval is still wide. My guess is that in my case, after 11 iterations the generated interval I<sub>11</sub> is such that a trial point \u03b1<sub>11</sub> respects both conditions. But, even though I<sub>12</sub> is smaller and still containing acceptable points, \u03b1<sub>12</sub> is not. Finally after 24 iterations, the interval is very small and the generated \u03b1<sub>k</sub> respects the update conditions.</p>\n<p>Is my understanding / explanation accurate?\nIf so, I would then be surprised that when <code>maxls=12</code>, since the generated \u03b1<sub>11</sub> is acceptable but not \u03b1<sub>12</sub>, why \u03b1<sub>11</sub> is not chosen in this case instead of \u03b1<sub>12</sub>?</p>\n<p>Pragmatically, I would recommend to try a few higher <code>maxls</code> when getting ABNORMAL_TERMINATION_IN_LNSRCH.</p>\n"
                }
            ],
            "owner": {
                "account_id": 1132467,
                "reputation": 3931,
                "user_id": 1118236,
                "user_type": "registered",
                "accept_rate": 41,
                "display_name": "Munichong"
            },
            "comment_count": 3,
            "is_answered": true,
            "accepted_answer_id": 39155976,
            "answer_count": 4,
            "score": 29,
            "last_activity_date": 1649666808,
            "creation_date": 1452194845,
            "last_edit_date": 1452197153,
            "question_id": 34663539,
            "link": "https://stackoverflow.com/questions/34663539/scipy-optimize-fmin-l-bfgs-b-returns-abnormal-termination-in-lnsrch",
            "title": "scipy.optimize.fmin_l_bfgs_b returns &#39;ABNORMAL_TERMINATION_IN_LNSRCH&#39;",
            "body": "<p>I am using scipy.optimize.fmin_l_bfgs_b to solve a gaussian mixture problem. The means of mixture distributions are modeled by regressions whose weights have to be optimized using EM algorithm.</p>\n\n<pre><code>sigma_sp_new, func_val, info_dict = fmin_l_bfgs_b(func_to_minimize, self.sigma_vector[si][pj], \n                       args=(self.w_vectors[si][pj], Y, X, E_step_results[si][pj]),\n                       approx_grad=True, bounds=[(1e-8, 0.5)], factr=1e02, pgtol=1e-05, epsilon=1e-08)\n</code></pre>\n\n<p>But sometimes I got a warning 'ABNORMAL_TERMINATION_IN_LNSRCH' in the information dictionary: </p>\n\n<pre><code>func_to_minimize value = 1.14462324063e-07\ninformation dictionary: {'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 147, 'grad': array([  1.77635684e-05,   2.87769808e-05,   3.51718654e-05,\n         6.75015599e-06,  -4.97379915e-06,  -1.06581410e-06]), 'nit': 0, 'warnflag': 2}\n\nRUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            6     M =           10\n This problem is unconstrained.\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  1.14462D-07    |proj g|=  3.51719D-05\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    6      1     21      1     0     0   3.517D-05   1.145D-07\n  F =  1.144619474757747E-007\n\nABNORMAL_TERMINATION_IN_LNSRCH                              \n\n Line search cannot locate an adequate point after 20 function\n  and gradient evaluations.  Previous x, f and g restored.\n Possible causes: 1 error in function or gradient evaluation;\n                  2 rounding error dominate computation.\n\n Cauchy                time 0.000E+00 seconds.\n Subspace minimization time 0.000E+00 seconds.\n Line search           time 0.000E+00 seconds.\n\n Total User time 0.000E+00 seconds.\n</code></pre>\n\n<p>I do not get this warning every time, but sometimes. (Most get 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL' or 'CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH').</p>\n\n<p>I know that it means the minimum can be be reached in this iteration. I googled this problem. Someone said it occurs often because the objective and gradient functions do not match. But here I do not provide gradient function because I am using 'approx_grad'.</p>\n\n<p>What are the possible reasons that I should investigate? What does it mean by \"rounding error dominate computation\"?</p>\n\n<p>======</p>\n\n<p>I also find that the log-likelihood does not monotonically increase:</p>\n\n<pre><code>########## Convergence !!! ##########\nlog_likelihood_history: [-28659.725891322563, 220.49993177669558, 291.3513633060345, 267.47745327823907, 265.31567762171181, 265.07311121000367, 265.04217683341682]\n</code></pre>\n\n<p>It usually start decrease at the second or the third iteration, even through 'ABNORMAL_TERMINATION_IN_LNSRCH' does not occurs. I do not know whether it this problem is related to the previous one.</p>\n"
        }
    ],
    "has_more": false,
    "quota_max": 10000,
    "quota_remaining": 9877
}