{
    "items": [
        {
            "tags": [
                "python",
                "tensorflow"
            ],
            "comments": [
                {
                    "owner": {
                        "account_id": 1589784,
                        "reputation": 6089,
                        "user_id": 1601580,
                        "user_type": "registered",
                        "accept_rate": 47,
                        "display_name": "Charlie Parker"
                    },
                    "edited": false,
                    "score": 1,
                    "creation_date": 1466291102,
                    "post_id": 33949786,
                    "comment_id": 63257757,
                    "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow#comment63257757_33949786",
                    "body": "I believe it has a documentation now: <a href=\"https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#batch_normalization\" rel=\"nofollow noreferrer\">tensorflow.org/versions/r0.9/api_docs/python/&hellip;</a>"
                },
                {
                    "owner": {
                        "account_id": 1589784,
                        "reputation": 6089,
                        "user_id": 1601580,
                        "user_type": "registered",
                        "accept_rate": 47,
                        "display_name": "Charlie Parker"
                    },
                    "edited": false,
                    "score": 0,
                    "creation_date": 1466446273,
                    "post_id": 33949786,
                    "comment_id": 63308873,
                    "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow#comment63308873_33949786",
                    "body": "I don&#39;t think there is such an tf.Op (BatchNormWithGlobalNormalization) anymore"
                },
                {
                    "owner": {
                        "account_id": 1589784,
                        "reputation": 6089,
                        "user_id": 1601580,
                        "user_type": "registered",
                        "accept_rate": 47,
                        "display_name": "Charlie Parker"
                    },
                    "edited": false,
                    "score": 0,
                    "creation_date": 1468267003,
                    "post_id": 33949786,
                    "comment_id": 64046751,
                    "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow#comment64046751_33949786",
                    "body": "I think there is an official BN layer <a href=\"https://github.com/tensorflow/tensorflow/blob/b826b79718e3e93148c3545e7aa3f90891744cc0/tensorflow/contrib/layers/python/layers/layers.py#L100\" rel=\"nofollow noreferrer\">github.com/tensorflow/tensorflow/blob/&hellip;</a>"
                },
                {
                    "owner": {
                        "account_id": 271958,
                        "reputation": 131586,
                        "user_id": 562769,
                        "user_type": "registered",
                        "accept_rate": 61,
                        "display_name": "Martin Thoma"
                    },
                    "edited": false,
                    "score": 0,
                    "creation_date": 1488306768,
                    "post_id": 33949786,
                    "comment_id": 72169491,
                    "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow#comment72169491_33949786",
                    "body": "I came here when I looked for <a href=\"http://stackoverflow.com/a/42325368/562769\">How to use Batch Normalization with tflearn</a>"
                }
            ],
            "answers": [
                {
                    "comments": [
                        {
                            "owner": {
                                "account_id": 1041567,
                                "reputation": 2309,
                                "user_id": 1047506,
                                "user_type": "registered",
                                "display_name": "Joren Van Severen"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1448895035,
                            "post_id": 33950177,
                            "comment_id": 55758348,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment55758348_33950177",
                            "body": "I&#39;m having a hard time applying this to a convnet subgraph that I&#39;m reusing in my LSTM network. By default it&#39;s creating a different normalizer for every timestep the subgraph is applied. Any ideas to make it normalize over all applications of the subgraph?"
                        },
                        {
                            "owner": {
                                "account_id": 7272904,
                                "reputation": 21847,
                                "user_id": 5545260,
                                "user_type": "registered",
                                "display_name": "dga"
                            },
                            "reply_to_user": {
                                "account_id": 1041567,
                                "reputation": 2309,
                                "user_id": 1047506,
                                "user_type": "registered",
                                "display_name": "Joren Van Severen"
                            },
                            "edited": false,
                            "score": 1,
                            "creation_date": 1448911227,
                            "post_id": 33950177,
                            "comment_id": 55768299,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment55768299_33950177",
                            "body": "Did you try creating the bn outside the subgraph and passing it in to the subgraph constructor?  <code>bn = Conv...er(args); ... createSubgraph(bn, args);</code> and then just invoke <code>bn.normalize</code> inside the subgraph."
                        },
                        {
                            "owner": {
                                "account_id": 1311467,
                                "reputation": 2431,
                                "user_id": 1259390,
                                "user_type": "registered",
                                "accept_rate": 44,
                                "display_name": "jrabary"
                            },
                            "edited": false,
                            "score": 1,
                            "creation_date": 1448955013,
                            "post_id": 33950177,
                            "comment_id": 55785049,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment55785049_33950177",
                            "body": "I don&#39;t understand why in this example do you compute the moving average during the test phase ?"
                        },
                        {
                            "owner": {
                                "account_id": 7272904,
                                "reputation": 21847,
                                "user_id": 5545260,
                                "user_type": "registered",
                                "display_name": "dga"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1448996671,
                            "post_id": 33950177,
                            "comment_id": 55811398,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment55811398_33950177",
                            "body": "The opposite - during training (<code>if train:</code>), it computes the mean and stddev of the input batch (<code>tf.nn.moments(x, [0, 1, 2])</code>).  During evaluation/testing, it extracts the saved moving average (<code>self.ewma_trainer.average(self.mean)</code>).  The confusing thing may be that calling the ewma&#39;s <code>average</code> method <i>returns the stored average</i>, it doesn&#39;t update it.  The update is done by the <code>self.mean.assign(mean)</code> line, which stores the current batch mean into &#39;self.mean&#39;, and then the <code>ewma_trainer.apply</code>, which updates the EWMA based upon <code>self.mean</code>"
                        },
                        {
                            "owner": {
                                "account_id": 1041567,
                                "reputation": 2309,
                                "user_id": 1047506,
                                "user_type": "registered",
                                "display_name": "Joren Van Severen"
                            },
                            "edited": false,
                            "score": 1,
                            "creation_date": 1449346480,
                            "post_id": 33950177,
                            "comment_id": 55970443,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment55970443_33950177",
                            "body": "@dga: yes I did and it ran (caused an error before), but I was seeing weird behavior. I&#39;m building graph twice as in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py\" rel=\"nofollow noreferrer\">github.com/tensorflow/tensorflow/blob/master/tensorflow/mode&zwnj;&#8203;ls/&hellip;</a> and use the second one for testing on bigger train &amp; valid batches. With batch normalization I&#39;m getting increasing/random loss &amp; acc. for the second graph, while the first one, used for the training op, shows a nice decreasing loss."
                        },
                        {
                            "owner": {
                                "account_id": 7272904,
                                "reputation": 21847,
                                "user_id": 5545260,
                                "user_type": "registered",
                                "display_name": "dga"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1449360900,
                            "post_id": 33950177,
                            "comment_id": 55974793,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment55974793_33950177",
                            "body": "That model has dropout - did you remove it?  <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py#L101\" rel=\"nofollow noreferrer\">github.com/tensorflow/tensorflow/blob/master/tensorflow/mode&zwnj;&#8203;ls/&hellip;</a> I&#39;ve found that BN and dropout can together add too much noise, preventing convergence.  (Note that there&#39;s a second dropout a few lines later).  But the other question is to check that the averaged mean and variance look right - if you didn&#39;t save them with the control dependencies, then testing won&#39;t work right, even though training will progress."
                        },
                        {
                            "owner": {
                                "account_id": 459393,
                                "reputation": 2052,
                                "user_id": 859840,
                                "user_type": "registered",
                                "accept_rate": 70,
                                "display_name": "richizy"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1453342125,
                            "post_id": 33950177,
                            "comment_id": 57563615,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment57563615_33950177",
                            "body": "What is the use of <code>tf.identity</code> in <code>local_gamma</code> and <code>local_beta</code>? I don&#39;t see any control dependencies within <code>normalize</code>&#39;s else-branch.."
                        },
                        {
                            "owner": {
                                "account_id": 7272904,
                                "reputation": 21847,
                                "user_id": 5545260,
                                "user_type": "registered",
                                "display_name": "dga"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1453398306,
                            "post_id": 33950177,
                            "comment_id": 57594378,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment57594378_33950177",
                            "body": "I wrote it in the context of a multi-GPU training example and wanted to be sure the variables were stored locally to the GPU.  Since that time, we&#39;ve changed the semantics of the variable access a bit (they get copied by default), so those lines are probably redundant."
                        },
                        {
                            "owner": {
                                "account_id": 2625757,
                                "reputation": 1226,
                                "user_id": 2272798,
                                "user_type": "registered",
                                "accept_rate": 78,
                                "display_name": "jstaker7"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1455133259,
                            "post_id": 33950177,
                            "comment_id": 58357679,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment58357679_33950177",
                            "body": "Why is <code>tf.train.ExponentialMovingAverage</code> created outside of the class? There isn&#39;t any reason why it couldn&#39;t be created in <code>__init__</code>, is that correct?"
                        },
                        {
                            "owner": {
                                "account_id": 2625757,
                                "reputation": 1226,
                                "user_id": 2272798,
                                "user_type": "registered",
                                "accept_rate": 78,
                                "display_name": "jstaker7"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1457552528,
                            "post_id": 33950177,
                            "comment_id": 59462591,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment59462591_33950177",
                            "body": "I&#39;m having trouble getting the same results on my test set compared to the validation set, which is evaluated during training. I think I&#39;ve narrowed this down to normalization not behaving properly when the model is loaded. Is there any obvious pitfall I might be encountering? Could this be related to resetting self.variance and self.mean when re-loading the model checkpoint?"
                        },
                        {
                            "owner": {
                                "account_id": 2651164,
                                "reputation": 938,
                                "user_id": 2292800,
                                "user_type": "registered",
                                "accept_rate": 75,
                                "display_name": "Mark Woodward"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1462410496,
                            "post_id": 33950177,
                            "comment_id": 61630957,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment61630957_33950177",
                            "body": "@dga, just curious, why is there a separate method wrapping <code>self.ema.apply(...)</code>? Couldn&#39;t we immediately apply <code>self.mean</code> and <code>self.variance</code> with a nested <code>tf.control_dependency</code> inside the <code>assign_mean</code>, <code>assign_variance</code> <code>tf.control_dependency</code> context. Thanks"
                        },
                        {
                            "owner": {
                                "account_id": 1589784,
                                "reputation": 6089,
                                "user_id": 1601580,
                                "user_type": "registered",
                                "accept_rate": 47,
                                "display_name": "Charlie Parker"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1466432666,
                            "post_id": 33950177,
                            "comment_id": 63300450,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment63300450_33950177",
                            "body": "sorry if this is a dum question but there doesn&#39;t seem to be any <code>tf.nn. BatchNormWithGlobalNormalization</code> anymore...is it suppose to be <code>tf.nn.batch_normalization</code>?"
                        },
                        {
                            "owner": {
                                "account_id": 1589784,
                                "reputation": 6089,
                                "user_id": 1601580,
                                "user_type": "registered",
                                "accept_rate": 47,
                                "display_name": "Charlie Parker"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1466434293,
                            "post_id": 33950177,
                            "comment_id": 63301568,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment63301568_33950177",
                            "body": "Also, what is the <code>scale_after_norm</code> after norm suppose to be doing?"
                        },
                        {
                            "owner": {
                                "account_id": 7272904,
                                "reputation": 21847,
                                "user_id": 5545260,
                                "user_type": "registered",
                                "display_name": "dga"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1466483084,
                            "post_id": 33950177,
                            "comment_id": 63320930,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment63320930_33950177",
                            "body": "Yes - it&#39;s now an official part of the API and is accessible through the normal tf.nn namespace now.  The scale_after_norm parameter is no longer applicable.  I should update the example wrapper one of these days.  Contributions welcome. :)"
                        },
                        {
                            "owner": {
                                "account_id": 1589784,
                                "reputation": 6089,
                                "user_id": 1601580,
                                "user_type": "registered",
                                "accept_rate": 47,
                                "display_name": "Charlie Parker"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1468268228,
                            "post_id": 33950177,
                            "comment_id": 64047353,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment64047353_33950177",
                            "body": "this answer is really outdated, there is an &quot;official&quot; way to add the BN layer, see here: <a href=\"https://github.com/tensorflow/tensorflow/blob/b826b79718e3e93148c3545e7aa3f90891744cc0/tensorflow/contrib/layers/python/layers/layers.py#L100\" rel=\"nofollow noreferrer\">github.com/tensorflow/tensorflow/blob/&hellip;</a>"
                        },
                        {
                            "owner": {
                                "account_id": 7272904,
                                "reputation": 21847,
                                "user_id": 5545260,
                                "user_type": "registered",
                                "display_name": "dga"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1468531023,
                            "post_id": 33950177,
                            "comment_id": 64178726,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment64178726_33950177",
                            "body": "Concur, if you&#39;re using the higher-layer support in contrib.  I&#39;ll update the answer to point to that - thanks for the nudge."
                        },
                        {
                            "owner": {
                                "account_id": 7272904,
                                "reputation": 21847,
                                "user_id": 5545260,
                                "user_type": "registered",
                                "display_name": "dga"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1468880170,
                            "post_id": 33950177,
                            "comment_id": 64299001,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment64299001_33950177",
                            "body": "For scale_after_norm, see the last line in the description of algorithm 1 in the batchnorm paper:  <a href=\"https://arxiv.org/pdf/1502.03167v3.pdf\" rel=\"nofollow noreferrer\">arxiv.org/pdf/1502.03167v3.pdf</a>"
                        },
                        {
                            "owner": {
                                "account_id": 1589784,
                                "reputation": 6089,
                                "user_id": 1601580,
                                "user_type": "registered",
                                "accept_rate": 47,
                                "display_name": "Charlie Parker"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1485468875,
                            "post_id": 33950177,
                            "comment_id": 70950283,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#comment70950283_33950177",
                            "body": "sorry for being picky about your answer but since it has the most views I feel obliged to comment. At the top you recommend slim, tflearn etc. You should probably recommend keras above those since keras is the official high level API for TensorFlow."
                        }
                    ],
                    "owner": {
                        "account_id": 7272904,
                        "reputation": 21847,
                        "user_id": 5545260,
                        "user_type": "registered",
                        "display_name": "dga"
                    },
                    "comment_count": 18,
                    "is_accepted": true,
                    "score": 57,
                    "last_activity_date": 1468531261,
                    "last_edit_date": 1468531261,
                    "creation_date": 1448597771,
                    "answer_id": 33950177,
                    "question_id": 33949786,
                    "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#33950177",
                    "body": "<p><strong>Update July 2016</strong>  The easiest way to use batch normalization in TensorFlow is through the higher-level interfaces provided in either <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py\" rel=\"noreferrer\">contrib/layers</a>, <a href=\"http://tflearn.org/layers/normalization/\" rel=\"noreferrer\">tflearn</a>, or <a href=\"https://github.com/tensorflow/models/blob/master/inception/inception/slim/ops.py\" rel=\"noreferrer\">slim</a>.</p>\n\n<p><strong>Previous answer if you want to DIY</strong>:\nThe documentation string for this has improved since the release - see the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L65\" rel=\"noreferrer\">docs comment in the master branch</a> instead of the one you found.  It clarifies, in particular, that it's the output from <code>tf.nn.moments</code>.</p>\n\n<p>You can see a very simple example of its use in the <a href=\"https://github.com/tensorflow/tensorflow/blob/3972c791b9f4d9a61b9ad6399b481df396f359ff/tensorflow/python/ops/nn_test.py#L518\" rel=\"noreferrer\">batch_norm test code</a>.  For a more real-world use example, I've included below the helper class and use notes that I scribbled up for my own use (no warranty provided!):</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>\"\"\"A helper class for managing batch normalization state.                   \n\nThis class is designed to simplify adding batch normalization               \n(http://arxiv.org/pdf/1502.03167v3.pdf) to your model by                    \nmanaging the state variables associated with it.                            \n\nImportant use note:  The function get_assigner() returns                    \nan op that must be executed to save the updated state.                      \nA suggested way to do this is to make execution of the                      \nmodel optimizer force it, e.g., by:                                         \n\n  update_assignments = tf.group(bn1.get_assigner(),                         \n                                bn2.get_assigner())                         \n  with tf.control_dependencies([optimizer]):                                \n    optimizer = tf.group(update_assignments)                                \n\n\"\"\"\n\nimport tensorflow as tf\n\n\nclass ConvolutionalBatchNormalizer(object):\n  \"\"\"Helper class that groups the normalization logic and variables.        \n\n  Use:                                                                      \n      ewma = tf.train.ExponentialMovingAverage(decay=0.99)                  \n      bn = ConvolutionalBatchNormalizer(depth, 0.001, ewma, True)           \n      update_assignments = bn.get_assigner()                                \n      x = bn.normalize(y, train=training?)                                  \n      (the output x will be batch-normalized).                              \n  \"\"\"\n\n  def __init__(self, depth, epsilon, ewma_trainer, scale_after_norm):\n    self.mean = tf.Variable(tf.constant(0.0, shape=[depth]),\n                            trainable=False)\n    self.variance = tf.Variable(tf.constant(1.0, shape=[depth]),\n                                trainable=False)\n    self.beta = tf.Variable(tf.constant(0.0, shape=[depth]))\n    self.gamma = tf.Variable(tf.constant(1.0, shape=[depth]))\n    self.ewma_trainer = ewma_trainer\n    self.epsilon = epsilon\n    self.scale_after_norm = scale_after_norm\n\n  def get_assigner(self):\n    \"\"\"Returns an EWMA apply op that must be invoked after optimization.\"\"\"\n    return self.ewma_trainer.apply([self.mean, self.variance])\n\n  def normalize(self, x, train=True):\n    \"\"\"Returns a batch-normalized version of x.\"\"\"\n    if train:\n      mean, variance = tf.nn.moments(x, [0, 1, 2])\n      assign_mean = self.mean.assign(mean)\n      assign_variance = self.variance.assign(variance)\n      with tf.control_dependencies([assign_mean, assign_variance]):\n        return tf.nn.batch_norm_with_global_normalization(\n            x, mean, variance, self.beta, self.gamma,\n            self.epsilon, self.scale_after_norm)\n    else:\n      mean = self.ewma_trainer.average(self.mean)\n      variance = self.ewma_trainer.average(self.variance)\n      local_beta = tf.identity(self.beta)\n      local_gamma = tf.identity(self.gamma)\n      return tf.nn.batch_norm_with_global_normalization(\n          x, mean, variance, local_beta, local_gamma,\n          self.epsilon, self.scale_after_norm)\n</code></pre>\n\n<p>Note that I called it a <code>ConvolutionalBatchNormalizer</code> because it pins the use of <code>tf.nn.moments</code> to sum across axes 0, 1, and 2, whereas for non-convolutional use you might only want axis 0.</p>\n\n<p>Feedback appreciated if you use it.</p>\n"
                },
                {
                    "comments": [
                        {
                            "owner": {
                                "account_id": 3714312,
                                "reputation": 1003,
                                "user_id": 3090897,
                                "user_type": "registered",
                                "display_name": "Shawn Lee"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1452393360,
                            "post_id": 34634291,
                            "comment_id": 57148905,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/34634291#comment57148905_34634291",
                            "body": "Thanks for another answer :). What is your <code>control_flow_ops.cond</code>? Is it <code>tf.control_flow_ops.cond</code>? I did not find it in tensorflow. Have you considered the performance difference? Since if the control dependency is applied in layer, then maybe the computation has to wait for every layer instead of wait for every iteration, and could it be too much waiting? I actually use your version, the in layer one, since it is simpler, but I will try the global one later."
                        },
                        {
                            "owner": {
                                "account_id": 4464582,
                                "reputation": 1174,
                                "user_id": 3632556,
                                "user_type": "registered",
                                "display_name": "bgshi"
                            },
                            "reply_to_user": {
                                "account_id": 3714312,
                                "reputation": 1003,
                                "user_id": 3090897,
                                "user_type": "registered",
                                "display_name": "Shawn Lee"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1452434203,
                            "post_id": 34634291,
                            "comment_id": 57158253,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/34634291#comment57158253_34634291",
                            "body": "I have updated the answer. It is tensorflow.python.control_flow_ops, which is not documented yet. I guess the EMA-apply would not cost much time, since it is an element-wise operation on a vector whose length is typically a few hundred. But I have not verified this yet."
                        },
                        {
                            "owner": {
                                "account_id": 1944151,
                                "reputation": 462,
                                "user_id": 1749667,
                                "user_type": "registered",
                                "display_name": "myme5261314"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1462349981,
                            "post_id": 34634291,
                            "comment_id": 61594981,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/34634291#comment61594981_34634291",
                            "body": "I&#39;ve confirmed what @jrocks said in his answer, your code is kind of buggy. Please notice."
                        },
                        {
                            "owner": {
                                "account_id": 4464582,
                                "reputation": 1174,
                                "user_id": 3632556,
                                "user_type": "registered",
                                "display_name": "bgshi"
                            },
                            "reply_to_user": {
                                "account_id": 1944151,
                                "reputation": 462,
                                "user_id": 1749667,
                                "user_type": "registered",
                                "display_name": "myme5261314"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1462503609,
                            "post_id": 34634291,
                            "comment_id": 61673388,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/34634291#comment61673388_34634291",
                            "body": "@myme5261314 @jrock You are right, looks like <code>ema_apply_op</code> is also called during testing. I have edited my answer, changing <code>phase_train</code> from a <code>tf.Variable</code> to a python boolean. However, now you have to create separate graphs for training and testing. Thanks for your feedback and sorry for my late response."
                        },
                        {
                            "owner": {
                                "account_id": 4464582,
                                "reputation": 1174,
                                "user_id": 3632556,
                                "user_type": "registered",
                                "display_name": "bgshi"
                            },
                            "reply_to_user": {
                                "account_id": 1944151,
                                "reputation": 462,
                                "user_id": 1749667,
                                "user_type": "registered",
                                "display_name": "myme5261314"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1462546643,
                            "post_id": 34634291,
                            "comment_id": 61695655,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/34634291#comment61695655_34634291",
                            "body": "@myme5261314 @jrock I update my answer again. Turns out there was a bug in the original version that causes <code>ema_apply_op</code> being called even when <code>phase_train=False</code>."
                        },
                        {
                            "owner": {
                                "account_id": 1589784,
                                "reputation": 6089,
                                "user_id": 1601580,
                                "user_type": "registered",
                                "accept_rate": 47,
                                "display_name": "Charlie Parker"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1466432990,
                            "post_id": 34634291,
                            "comment_id": 63300675,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/34634291#comment63300675_34634291",
                            "body": "the other answers suggest to do set <code>trainable=False</code> for the mean and variance variables. Why is that not necessary in your code?"
                        },
                        {
                            "owner": {
                                "account_id": 4464582,
                                "reputation": 1174,
                                "user_id": 3632556,
                                "user_type": "registered",
                                "display_name": "bgshi"
                            },
                            "reply_to_user": {
                                "account_id": 1589784,
                                "reputation": 6089,
                                "user_id": 1601580,
                                "user_type": "registered",
                                "accept_rate": 47,
                                "display_name": "Charlie Parker"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1466443497,
                            "post_id": 34634291,
                            "comment_id": 63307422,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/34634291#comment63307422_34634291",
                            "body": "@Pinocchio mean and var are EMA shadow variables. According to the docs at <a href=\"https://www.tensorflow.org/versions/r0.9/api_docs/python/train.html#ExponentialMovingAverage\" rel=\"nofollow noreferrer\">tensorflow.org/versions/r0.9/api_docs/python/&hellip;</a>, &quot;shadow variables are created with <code>trainable=False</code>&quot;. So it is already set."
                        },
                        {
                            "owner": {
                                "account_id": 1589784,
                                "reputation": 6089,
                                "user_id": 1601580,
                                "user_type": "registered",
                                "accept_rate": 47,
                                "display_name": "Charlie Parker"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1466451246,
                            "post_id": 34634291,
                            "comment_id": 63311599,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/34634291#comment63311599_34634291",
                            "body": "why do you need the <code>with tf.control_dependencies([ema_apply_op]):</code> in your helper function used in training? If you want to just return the moments, why not simply have <code>return tf.identity(batch_mean), tf.identity(batch_var)</code> instead of this complicated with statement and reference to <code>ema</code>, that shouldn&#39;t be used during training?"
                        },
                        {
                            "owner": {
                                "account_id": 1589784,
                                "reputation": 6089,
                                "user_id": 1601580,
                                "user_type": "registered",
                                "accept_rate": 47,
                                "display_name": "Charlie Parker"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1466542256,
                            "post_id": 34634291,
                            "comment_id": 63357104,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/34634291#comment63357104_34634291",
                            "body": "Also, why do you have in the fetch <code>phase_train.name</code> rather than <code>phase_train</code>? Is that really necessary?"
                        },
                        {
                            "owner": {
                                "account_id": 4464582,
                                "reputation": 1174,
                                "user_id": 3632556,
                                "user_type": "registered",
                                "display_name": "bgshi"
                            },
                            "reply_to_user": {
                                "account_id": 1589784,
                                "reputation": 6089,
                                "user_id": 1601580,
                                "user_type": "registered",
                                "accept_rate": 47,
                                "display_name": "Charlie Parker"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1466575027,
                            "post_id": 34634291,
                            "comment_id": 63366535,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/34634291#comment63366535_34634291",
                            "body": "@Pinocchio Because batch normalization has different behaviors in training and testing. In training, it not only returns batch mean and variance but also updates the moving averages of both."
                        },
                        {
                            "owner": {
                                "account_id": 4464582,
                                "reputation": 1174,
                                "user_id": 3632556,
                                "user_type": "registered",
                                "display_name": "bgshi"
                            },
                            "reply_to_user": {
                                "account_id": 1589784,
                                "reputation": 6089,
                                "user_id": 1601580,
                                "user_type": "registered",
                                "accept_rate": 47,
                                "display_name": "Charlie Parker"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1466575270,
                            "post_id": 34634291,
                            "comment_id": 63366662,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/34634291#comment63366662_34634291",
                            "body": "@Pinocchio This was required in older TF versions. Seems no longer need to add <code>.name</code> in the latest version, but I have not verified this yet."
                        },
                        {
                            "owner": {
                                "account_id": 1589784,
                                "reputation": 6089,
                                "user_id": 1601580,
                                "user_type": "registered",
                                "accept_rate": 47,
                                "display_name": "Charlie Parker"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1468256196,
                            "post_id": 34634291,
                            "comment_id": 64041190,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/34634291#comment64041190_34634291",
                            "body": "during inference, phase_train is equal to False, right? Regardless of the data set (train data set, cv data set or test data set)?"
                        },
                        {
                            "owner": {
                                "account_id": 1589784,
                                "reputation": 6089,
                                "user_id": 1601580,
                                "user_type": "registered",
                                "accept_rate": 47,
                                "display_name": "Charlie Parker"
                            },
                            "edited": false,
                            "score": 3,
                            "creation_date": 1468267032,
                            "post_id": 34634291,
                            "comment_id": 64046769,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/34634291#comment64046769_34634291",
                            "body": "is your code really necessary considering there is an official BN layer? code: <a href=\"https://github.com/tensorflow/tensorflow/blob/b826b79718e3e93148c3545e7aa3f90891744cc0/tensorflow/contrib/layers/python/layers/layers.py#L100\" rel=\"nofollow noreferrer\">github.com/tensorflow/tensorflow/blob/&hellip;</a>"
                        },
                        {
                            "owner": {
                                "account_id": 1988078,
                                "reputation": 59116,
                                "user_id": 1782792,
                                "user_type": "registered",
                                "accept_rate": 100,
                                "display_name": "javidcf"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1496416350,
                            "post_id": 34634291,
                            "comment_id": 75668253,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/34634291#comment75668253_34634291",
                            "body": "@bgshi Great example. But I don&#39;t get why you won&#39;t use <code>ema.average</code> during training and instead just use batch averages. E.g. if you have a batch that is further from the mean than usual, wouldn&#39;t you rather train it with a normalization closer to what the network is actually going to use for prediction?"
                        },
                        {
                            "owner": {
                                "account_id": 1988078,
                                "reputation": 59116,
                                "user_id": 1782792,
                                "user_type": "registered",
                                "accept_rate": 100,
                                "display_name": "javidcf"
                            },
                            "reply_to_user": {
                                "account_id": 1589784,
                                "reputation": 6089,
                                "user_id": 1601580,
                                "user_type": "registered",
                                "accept_rate": 47,
                                "display_name": "Charlie Parker"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1496416504,
                            "post_id": 34634291,
                            "comment_id": 75668360,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/34634291#comment75668360_34634291",
                            "body": "@Pinocchio I personally find this useful to have a better control of the variables. E.g. with the BN layer I can&#39;t easily access the shadow variables, which is important for me (for exporting, save/restore, ...). Also <code>tf.nn.batch_normalization</code> allows you to normalize through multiple dimensions."
                        }
                    ],
                    "owner": {
                        "account_id": 4464582,
                        "reputation": 1174,
                        "user_id": 3632556,
                        "user_type": "registered",
                        "display_name": "bgshi"
                    },
                    "comment_count": 15,
                    "is_accepted": false,
                    "score": 32,
                    "last_activity_date": 1463608110,
                    "last_edit_date": 1463608110,
                    "creation_date": 1452086801,
                    "answer_id": 34634291,
                    "question_id": 33949786,
                    "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/34634291#34634291",
                    "body": "<p>The following works fine for me, it does not require invoking EMA-apply outside.</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python import control_flow_ops\n\ndef batch_norm(x, n_out, phase_train, scope='bn'):\n    \"\"\"\n    Batch normalization on convolutional maps.\n    Args:\n        x:           Tensor, 4D BHWD input maps\n        n_out:       integer, depth of input maps\n        phase_train: boolean tf.Varialbe, true indicates training phase\n        scope:       string, variable scope\n    Return:\n        normed:      batch-normalized maps\n    \"\"\"\n    with tf.variable_scope(scope):\n        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n                                     name='beta', trainable=True)\n        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n                                      name='gamma', trainable=True)\n        batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n\n        def mean_var_with_update():\n            ema_apply_op = ema.apply([batch_mean, batch_var])\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n\n        mean, var = tf.cond(phase_train,\n                            mean_var_with_update,\n                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n    return normed\n</code></pre>\n\n<p>Example:</p>\n\n<pre class=\"lang-python prettyprint-override\"><code>import math\n\nn_in, n_out = 3, 16\nksize = 3\nstride = 1\nphase_train = tf.placeholder(tf.bool, name='phase_train')\ninput_image = tf.placeholder(tf.float32, name='input_image')\nkernel = tf.Variable(tf.truncated_normal([ksize, ksize, n_in, n_out],\n                                   stddev=math.sqrt(2.0/(ksize*ksize*n_out))),\n                                   name='kernel')\nconv = tf.nn.conv2d(input_image, kernel, [1,stride,stride,1], padding='SAME')\nconv_bn = batch_norm(conv, n_out, phase_train)\nrelu = tf.nn.relu(conv_bn)\n\nwith tf.Session() as session:\n    session.run(tf.initialize_all_variables())\n    for i in range(20):\n        test_image = np.random.rand(4,32,32,3)\n        sess_outputs = session.run([relu],\n          {input_image.name: test_image, phase_train.name: True})\n</code></pre>\n"
                },
                {
                    "comments": [
                        {
                            "owner": {
                                "account_id": 4464582,
                                "reputation": 1174,
                                "user_id": 3632556,
                                "user_type": "registered",
                                "display_name": "bgshi"
                            },
                            "edited": false,
                            "score": 1,
                            "creation_date": 1462547110,
                            "post_id": 36511416,
                            "comment_id": 61695947,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/36511416#comment61695947_36511416",
                            "body": "I have updated my answer. There was a bug in the original version that causes <code>ema_apply_op</code> being called even when <code>phase_train=False</code>."
                        },
                        {
                            "owner": {
                                "account_id": 2283904,
                                "reputation": 245,
                                "user_id": 2008401,
                                "user_type": "registered",
                                "display_name": "jrock"
                            },
                            "reply_to_user": {
                                "account_id": 4464582,
                                "reputation": 1174,
                                "user_id": 3632556,
                                "user_type": "registered",
                                "display_name": "bgshi"
                            },
                            "edited": false,
                            "score": 2,
                            "creation_date": 1463010504,
                            "post_id": 36511416,
                            "comment_id": 61884707,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/36511416#comment61884707_36511416",
                            "body": "Thanks for the update, still can&#39;t comment on your thread (hurray for rep), but that looks like it should work now.  Thanks to @myme5261314 as well."
                        }
                    ],
                    "owner": {
                        "account_id": 2283904,
                        "reputation": 245,
                        "user_id": 2008401,
                        "user_type": "registered",
                        "display_name": "jrock"
                    },
                    "comment_count": 2,
                    "is_accepted": false,
                    "score": 11,
                    "last_activity_date": 1531330479,
                    "last_edit_date": 1531330479,
                    "creation_date": 1460162161,
                    "answer_id": 36511416,
                    "question_id": 33949786,
                    "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/36511416#36511416",
                    "body": "<p>Since someone recently edited this, I'd like to clarify that this is no longer an issue.</p>\n\n<p><a href=\"https://stackoverflow.com/a/34634291/3924118\">This answer</a> does not seem correct  When <code>phase_train</code> is set to false, it still updates the ema mean and variance. This can be verified with the following code snippet.</p>\n\n<pre><code>x = tf.placeholder(tf.float32, [None, 20, 20, 10], name='input')\nphase_train = tf.placeholder(tf.bool, name='phase_train')\n\n# generate random noise to pass into batch norm\nx_gen = tf.random_normal([50,20,20,10])\npt_false = tf.Variable(tf.constant(True))\n\n#generate a constant variable to pass into batch norm\ny = x_gen.eval()\n\n[bn, bn_vars] = batch_norm(x, 10, phase_train)\n\ntf.initialize_all_variables().run()\ntrain_step = lambda: bn.eval({x:x_gen.eval(), phase_train:True})\ntest_step = lambda: bn.eval({x:y, phase_train:False})\ntest_step_c = lambda: bn.eval({x:y, phase_train:True})\n\n# Verify that this is different as expected, two different x's have different norms\nprint(train_step()[0][0][0])\nprint(train_step()[0][0][0])\n\n# Verify that this is same as expected, same x's (y) have same norm\nprint(train_step_c()[0][0][0])\nprint(train_step_c()[0][0][0])\n\n# THIS IS DIFFERENT but should be they same, should only be reading from the ema.\nprint(test_step()[0][0][0])\nprint(test_step()[0][0][0])\n</code></pre>\n"
                },
                {
                    "owner": {
                        "account_id": 6233881,
                        "reputation": 843,
                        "user_id": 6104317,
                        "user_type": "registered",
                        "accept_rate": 33,
                        "display_name": "Rob Romijnders"
                    },
                    "comment_count": 0,
                    "is_accepted": false,
                    "score": 0,
                    "last_activity_date": 1460263427,
                    "creation_date": 1460263427,
                    "answer_id": 36525884,
                    "question_id": 33949786,
                    "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/36525884#36525884",
                    "body": "<p>So a simple example of the use of this batchnorm class:</p>\n\n<pre><code>from bn_class import *\n\nwith tf.name_scope('Batch_norm_conv1') as scope:\n    ewma = tf.train.ExponentialMovingAverage(decay=0.99)                  \n    bn_conv1 = ConvolutionalBatchNormalizer(num_filt_1, 0.001, ewma, True)           \n    update_assignments = bn_conv1.get_assigner() \n    a_conv1 = bn_conv1.normalize(a_conv1, train=bn_train) \n    h_conv1 = tf.nn.relu(a_conv1)\n</code></pre>\n"
                },
                {
                    "comments": [
                        {
                            "owner": {
                                "account_id": 1589784,
                                "reputation": 6089,
                                "user_id": 1601580,
                                "user_type": "registered",
                                "accept_rate": 47,
                                "display_name": "Charlie Parker"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1469725958,
                            "post_id": 38320613,
                            "comment_id": 64668586,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/38320613#comment64668586_38320613",
                            "body": "notice that <code>updates_collections=None</code> is important. I don&#39;t understand why but it is. The best explanation that I know is <code>But what it is important is that either you pass updates_collections=None so the moving_mean and moving_variance are updated in-place, otherwise you will need gather the update_ops and make sure they are run.</code> but I don&#39;t quite understand why that is an explanation but empirically I&#39;ve observed MNIST perform well when its None and terrible when its not."
                        }
                    ],
                    "owner": {
                        "account_id": 1589784,
                        "reputation": 6089,
                        "user_id": 1601580,
                        "user_type": "registered",
                        "accept_rate": 47,
                        "display_name": "Charlie Parker"
                    },
                    "comment_count": 1,
                    "is_accepted": false,
                    "score": 14,
                    "last_activity_date": 1531061058,
                    "last_edit_date": 1531061058,
                    "creation_date": 1468301036,
                    "answer_id": 38320613,
                    "question_id": 33949786,
                    "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/38320613#38320613",
                    "body": "<p>There is also an <a href=\"https://github.com/tensorflow/tensorflow/blob/b826b79718e3e93148c3545e7aa3f90891744cc0/tensorflow/contrib/layers/python/layers/layers.py#L100\" rel=\"nofollow noreferrer\">\"official\" batch normalization layer</a> coded by the developers. They don't have very good docs on how to use it but here is how to use it (according to me):</p>\n\n<pre><code>from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n\ndef batch_norm_layer(x,train_phase,scope_bn):\n    bn_train = batch_norm(x, decay=0.999, center=True, scale=True,\n    updates_collections=None,\n    is_training=True,\n    reuse=None, # is this right?\n    trainable=True,\n    scope=scope_bn)\n    bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,\n    updates_collections=None,\n    is_training=False,\n    reuse=True, # is this right?\n    trainable=True,\n    scope=scope_bn)\n    z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n    return z\n</code></pre>\n\n<p>to actually use it you need to create a placeholder for <code>train_phase</code> that indicates if you are in training or inference phase (as in <code>train_phase = tf.placeholder(tf.bool, name='phase_train')</code>). Its value can be filled during inference or training with a <code>tf.session</code> as in:</p>\n\n<pre><code>test_error = sess.run(fetches=cross_entropy, feed_dict={x: batch_xtest, y_:batch_ytest, train_phase: False})\n</code></pre>\n\n<p>or during training:</p>\n\n<pre><code>sess.run(fetches=train_step, feed_dict={x: batch_xs, y_:batch_ys, train_phase: True})\n</code></pre>\n\n<p>I'm pretty sure this is correct according to the discussion in <a href=\"https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-231917736\" rel=\"nofollow noreferrer\">github</a>.</p>\n\n<hr>\n\n<p>Seems there is another useful link:</p>\n\n<p><a href=\"http://r2rt.com/implementing-batch-normalization-in-tensorflow.html\" rel=\"nofollow noreferrer\">http://r2rt.com/implementing-batch-normalization-in-tensorflow.html</a></p>\n"
                },
                {
                    "comments": [
                        {
                            "owner": {
                                "account_id": 1589784,
                                "reputation": 6089,
                                "user_id": 1601580,
                                "user_type": "registered",
                                "accept_rate": 47,
                                "display_name": "Charlie Parker"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1469685768,
                            "post_id": 38325288,
                            "comment_id": 64640740,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/38325288#comment64640740_38325288",
                            "body": "how does one get the data set to try and run ur example? i.e. ` &#39;/home/maxkhk/Documents/Udacity/DeepLearningCourse/SourceCod&zwnj;&#8203;e/tensorflow/example&zwnj;&#8203;s/udacity/notMNIST.p&zwnj;&#8203;ickle&#39;  `"
                        },
                        {
                            "owner": {
                                "account_id": 4465484,
                                "reputation": 4782,
                                "user_id": 3633250,
                                "user_type": "registered",
                                "accept_rate": 100,
                                "display_name": "Maksim Khaitovich"
                            },
                            "reply_to_user": {
                                "account_id": 1589784,
                                "reputation": 6089,
                                "user_id": 1601580,
                                "user_type": "registered",
                                "accept_rate": 47,
                                "display_name": "Charlie Parker"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1469742722,
                            "post_id": 38325288,
                            "comment_id": 64677305,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/38325288#comment64677305_38325288",
                            "body": "@Pinocchio it is Udacity&#39;s course for Deep Learning and it is done in first assignment there, you could check my code for this here: <a href=\"https://github.com/MaxKHK/Udacity_DeepLearningAssignments/blob/master/Assignment1/1_notmnist.ipynb\" rel=\"nofollow noreferrer\">github.com/MaxKHK/Udacity_DeepLearningAssignments/blob/maste&zwnj;&#8203;r/&hellip;</a>"
                        },
                        {
                            "owner": {
                                "account_id": 1428723,
                                "reputation": 2949,
                                "user_id": 1351629,
                                "user_type": "registered",
                                "accept_rate": 75,
                                "display_name": "Temak"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1478133377,
                            "post_id": 38325288,
                            "comment_id": 68035556,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/38325288#comment68035556_38325288",
                            "body": "Seems like you don&#39;t update the moving averages of the batch_norm layer during training"
                        }
                    ],
                    "owner": {
                        "account_id": 4465484,
                        "reputation": 4782,
                        "user_id": 3633250,
                        "user_type": "registered",
                        "accept_rate": 100,
                        "display_name": "Maksim Khaitovich"
                    },
                    "comment_count": 3,
                    "is_accepted": false,
                    "score": 3,
                    "last_activity_date": 1468317209,
                    "last_edit_date": 1468317209,
                    "creation_date": 1468316706,
                    "answer_id": 38325288,
                    "question_id": 33949786,
                    "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/38325288#38325288",
                    "body": "<p>Using TensorFlow built-in batch_norm layer, below is the code to load data, build a network with one hidden ReLU layer and L2 normalization and introduce batch normalization for both hidden and out layer. This runs fine and trains fine. Just FYI this example is mostly built upon the data and code from Udacity DeepLearning course.\nP.S. Yes, parts of it were discussed one way or another in answers earlier but I decided to gather in one code snippet everything so that you have example of whole network training process with Batch Normalization and its evaluation</p>\n\n<pre><code># These are all the modules we'll be using later. Make sure you can import them\n# before proceeding further.\nfrom __future__ import print_function\nimport numpy as np\nimport tensorflow as tf\nfrom six.moves import cPickle as pickle\n\npickle_file = '/home/maxkhk/Documents/Udacity/DeepLearningCourse/SourceCode/tensorflow/examples/udacity/notMNIST.pickle'\n\nwith open(pickle_file, 'rb') as f:\n  save = pickle.load(f)\n  train_dataset = save['train_dataset']\n  train_labels = save['train_labels']\n  valid_dataset = save['valid_dataset']\n  valid_labels = save['valid_labels']\n  test_dataset = save['test_dataset']\n  test_labels = save['test_labels']\n  del save  # hint to help gc free up memory\n  print('Training set', train_dataset.shape, train_labels.shape)\n  print('Validation set', valid_dataset.shape, valid_labels.shape)\n  print('Test set', test_dataset.shape, test_labels.shape)\n\nimage_size = 28\nnum_labels = 10\n\ndef reformat(dataset, labels):\n  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n  return dataset, labels\ntrain_dataset, train_labels = reformat(train_dataset, train_labels)\nvalid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\ntest_dataset, test_labels = reformat(test_dataset, test_labels)\nprint('Training set', train_dataset.shape, train_labels.shape)\nprint('Validation set', valid_dataset.shape, valid_labels.shape)\nprint('Test set', test_dataset.shape, test_labels.shape)\n\n\ndef accuracy(predictions, labels):\n  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n          / predictions.shape[0])\n\n\n#for NeuralNetwork model code is below\n#We will use SGD for training to save our time. Code is from Assignment 2\n#beta is the new parameter - controls level of regularization.\n#Feel free to play with it - the best one I found is 0.001\n#notice, we introduce L2 for both biases and weights of all layers\n\nbatch_size = 128\nbeta = 0.001\n\n#building tensorflow graph\ngraph = tf.Graph()\nwith graph.as_default():\n      # Input data. For the training data, we use a placeholder that will be fed\n  # at run time with a training minibatch.\n  tf_train_dataset = tf.placeholder(tf.float32,\n                                    shape=(batch_size, image_size * image_size))\n  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n  tf_valid_dataset = tf.constant(valid_dataset)\n  tf_test_dataset = tf.constant(test_dataset)\n\n  #introduce batchnorm\n  tf_train_dataset_bn = tf.contrib.layers.batch_norm(tf_train_dataset)\n\n\n  #now let's build our new hidden layer\n  #that's how many hidden neurons we want\n  num_hidden_neurons = 1024\n  #its weights\n  hidden_weights = tf.Variable(\n    tf.truncated_normal([image_size * image_size, num_hidden_neurons]))\n  hidden_biases = tf.Variable(tf.zeros([num_hidden_neurons]))\n\n  #now the layer itself. It multiplies data by weights, adds biases\n  #and takes ReLU over result\n  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset_bn, hidden_weights) + hidden_biases)\n\n  #adding the batch normalization layerhi()\n  hidden_layer_bn = tf.contrib.layers.batch_norm(hidden_layer)\n\n  #time to go for output linear layer\n  #out weights connect hidden neurons to output labels\n  #biases are added to output labels  \n  out_weights = tf.Variable(\n    tf.truncated_normal([num_hidden_neurons, num_labels]))  \n\n  out_biases = tf.Variable(tf.zeros([num_labels]))  \n\n  #compute output  \n  out_layer = tf.matmul(hidden_layer_bn,out_weights) + out_biases\n  #our real output is a softmax of prior result\n  #and we also compute its cross-entropy to get our loss\n  #Notice - we introduce our L2 here\n  loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    out_layer, tf_train_labels) +\n    beta*tf.nn.l2_loss(hidden_weights) +\n    beta*tf.nn.l2_loss(hidden_biases) +\n    beta*tf.nn.l2_loss(out_weights) +\n    beta*tf.nn.l2_loss(out_biases)))\n\n  #now we just minimize this loss to actually train the network\n  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\n  #nice, now let's calculate the predictions on each dataset for evaluating the\n  #performance so far\n  # Predictions for the training, validation, and test data.\n  train_prediction = tf.nn.softmax(out_layer)\n  valid_relu = tf.nn.relu(  tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)\n  valid_prediction = tf.nn.softmax( tf.matmul(valid_relu, out_weights) + out_biases) \n\n  test_relu = tf.nn.relu( tf.matmul( tf_test_dataset, hidden_weights) + hidden_biases)\n  test_prediction = tf.nn.softmax(tf.matmul(test_relu, out_weights) + out_biases)\n\n\n\n#now is the actual training on the ANN we built\n#we will run it for some number of steps and evaluate the progress after \n#every 500 steps\n\n#number of steps we will train our ANN\nnum_steps = 3001\n\n#actual training\nwith tf.Session(graph=graph) as session:\n  tf.initialize_all_variables().run()\n  print(\"Initialized\")\n  for step in range(num_steps):\n    # Pick an offset within the training data, which has been randomized.\n    # Note: we could use better randomization across epochs.\n    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n    # Generate a minibatch.\n    batch_data = train_dataset[offset:(offset + batch_size), :]\n    batch_labels = train_labels[offset:(offset + batch_size), :]\n    # Prepare a dictionary telling the session where to feed the minibatch.\n    # The key of the dictionary is the placeholder node of the graph to be fed,\n    # and the value is the numpy array to feed to it.\n    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n    _, l, predictions = session.run(\n      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n    if (step % 500 == 0):\n      print(\"Minibatch loss at step %d: %f\" % (step, l))\n      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n      print(\"Validation accuracy: %.1f%%\" % accuracy(\n        valid_prediction.eval(), valid_labels))\n      print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n</code></pre>\n"
                },
                {
                    "comments": [
                        {
                            "owner": {
                                "account_id": 2208620,
                                "reputation": 4627,
                                "user_id": 1951176,
                                "user_type": "registered",
                                "accept_rate": 71,
                                "display_name": "sygi"
                            },
                            "edited": false,
                            "score": 1,
                            "creation_date": 1473967044,
                            "post_id": 39128711,
                            "comment_id": 66353171,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/39128711#comment66353171_39128711",
                            "body": "Do you have an example, where you don&#39;t pass <code>is_train</code> as a placeholder? I can&#39;t do that, passing python boolean don&#39;t work with <code>tf.cond</code> and defining two batch norms in if branches gives me &quot;reuse=True cannot be used without a name_or_scope&quot; (even when I specify a variable scope for them)..."
                        },
                        {
                            "owner": {
                                "account_id": 10716056,
                                "reputation": 2312,
                                "user_id": 7886651,
                                "user_type": "registered",
                                "accept_rate": 76,
                                "display_name": "I. A"
                            },
                            "reply_to_user": {
                                "account_id": 2208620,
                                "reputation": 4627,
                                "user_id": 1951176,
                                "user_type": "registered",
                                "accept_rate": 71,
                                "display_name": "sygi"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1502239078,
                            "post_id": 39128711,
                            "comment_id": 78119333,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/39128711#comment78119333_39128711",
                            "body": "@sygi, you can use <code>tf.cast(True&#47;False, tf.bool)</code> operation."
                        },
                        {
                            "owner": {
                                "account_id": 10716056,
                                "reputation": 2312,
                                "user_id": 7886651,
                                "user_type": "registered",
                                "accept_rate": 76,
                                "display_name": "I. A"
                            },
                            "reply_to_user": {
                                "account_id": 2208620,
                                "reputation": 4627,
                                "user_id": 1951176,
                                "user_type": "registered",
                                "accept_rate": 71,
                                "display_name": "sygi"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1502307759,
                            "post_id": 39128711,
                            "comment_id": 78158206,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/39128711#comment78158206_39128711",
                            "body": "@sygi, Yea I know, you can say for example: <code>var1 = True or False</code>, and then say: <code>tf.cast(var1, tf.bool)</code>. This should work just fine"
                        },
                        {
                            "owner": {
                                "account_id": 8078276,
                                "reputation": 167,
                                "user_id": 6088463,
                                "user_type": "registered",
                                "accept_rate": 67,
                                "display_name": "JenkinsY"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1541490582,
                            "post_id": 39128711,
                            "comment_id": 93225195,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/39128711#comment93225195_39128711",
                            "body": "why do you set <code>reuse=True</code> in and only in test stage?"
                        }
                    ],
                    "owner": {
                        "account_id": 9069944,
                        "reputation": 119,
                        "user_id": 6753555,
                        "user_type": "registered",
                        "display_name": "Martina Marek"
                    },
                    "comment_count": 4,
                    "is_accepted": false,
                    "score": 11,
                    "last_activity_date": 1472160925,
                    "last_edit_date": 1472160925,
                    "creation_date": 1472056512,
                    "answer_id": 39128711,
                    "question_id": 33949786,
                    "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/39128711#39128711",
                    "body": "<p>You can simply use the build-in batch_norm layer:</p>\n\n<pre><code>batch_norm = tf.cond(is_train, \n    lambda: tf.contrib.layers.batch_norm(prev, activation_fn=tf.nn.relu, is_training=True, reuse=None),\n    lambda: tf.contrib.layers.batch_norm(prev, activation_fn =tf.nn.relu, is_training=False, reuse=True))\n</code></pre>\n\n<p>where prev is the output of your previous layer (can be both fully-connected or a convolutional layer) and is_train is a boolean placeholder. Just use batch_norm as the input to the next layer, then.</p>\n"
                },
                {
                    "comments": [
                        {
                            "owner": {
                                "account_id": 9728606,
                                "reputation": 1109,
                                "user_id": 7550636,
                                "user_type": "registered",
                                "accept_rate": 100,
                                "display_name": "mamafoku"
                            },
                            "reply_to_user": {
                                "account_id": 17306,
                                "reputation": 47633,
                                "user_id": 38626,
                                "user_type": "registered",
                                "accept_rate": 51,
                                "display_name": "MiniQuark"
                            },
                            "edited": false,
                            "score": 1,
                            "creation_date": 1502399567,
                            "post_id": 43285333,
                            "comment_id": 78206677,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/43285333#comment78206677_43285333",
                            "body": "@MiniQuark can you please elaborate on the dependencies? I don&#39;t quite understand that part."
                        },
                        {
                            "owner": {
                                "account_id": 17306,
                                "reputation": 47633,
                                "user_id": 38626,
                                "user_type": "registered",
                                "accept_rate": 51,
                                "display_name": "MiniQuark"
                            },
                            "reply_to_user": {
                                "account_id": 9728606,
                                "reputation": 1109,
                                "user_id": 7550636,
                                "user_type": "registered",
                                "accept_rate": 100,
                                "display_name": "mamafoku"
                            },
                            "edited": false,
                            "score": 5,
                            "creation_date": 1502445235,
                            "post_id": 43285333,
                            "comment_id": 78223283,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/43285333#comment78223283_43285333",
                            "body": "@mamafoku The Batch Norm algorithm needs to compute the mean and standard deviation of your whole training set.  These are <i>computed</i> during training, but they are not <i>used</i> during training, only during inference.  This computation is done using exponential averages.  It is independent from the rest of the training, so you must run the exponential average computation step (i.e. <code>extra_update_ops</code>) &quot;manually&quot; at each training iteration, along with the regular training op, or you can make the training op depend on <code>extra_update_ops</code> (using a <code>control_dependencies()</code> block). Hope this helps."
                        },
                        {
                            "owner": {
                                "account_id": 159364,
                                "reputation": 674,
                                "user_id": 378647,
                                "user_type": "registered",
                                "accept_rate": 67,
                                "display_name": "Andres Felipe"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1511269605,
                            "post_id": 43285333,
                            "comment_id": 81781026,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/43285333#comment81781026_43285333",
                            "body": "So considering that <code>update_ups</code> serves for the purpose of updating the moving mean and moving variance it would make no sense to include it at all if we are just testing a pre-trained network, is it correct?"
                        },
                        {
                            "owner": {
                                "account_id": 5895690,
                                "reputation": 10641,
                                "user_id": 4640905,
                                "user_type": "registered",
                                "accept_rate": 75,
                                "display_name": "Jonas Adler"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1519813586,
                            "post_id": 43285333,
                            "comment_id": 85058967,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/43285333#comment85058967_43285333",
                            "body": "What value for <code>axis</code> should be used in convolutional networks?"
                        },
                        {
                            "owner": {
                                "account_id": 10635317,
                                "reputation": 1133,
                                "user_id": 7832197,
                                "user_type": "registered",
                                "display_name": "Matthew Rahtz"
                            },
                            "reply_to_user": {
                                "account_id": 5895690,
                                "reputation": 10641,
                                "user_id": 4640905,
                                "user_type": "registered",
                                "accept_rate": 75,
                                "display_name": "Jonas Adler"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1519978275,
                            "post_id": 43285333,
                            "comment_id": 85135059,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/43285333#comment85135059_43285333",
                            "body": "@JonasAdler According to the documentation at <a href=\"https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\" rel=\"nofollow noreferrer\">tensorflow.org/api_docs/python/tf/layers/batch_normalization</a>&zwnj;&#8203;, <code>axis</code> should correspond to the channels axis. If you&#39;re using <code>tf.layers.conv2d</code> with the default <code>data_format</code> (<code>channels_last</code>), you can leave <code>axis</code> at the default, <code>-1</code>."
                        },
                        {
                            "owner": {
                                "account_id": 10635317,
                                "reputation": 1133,
                                "user_id": 7832197,
                                "user_type": "registered",
                                "display_name": "Matthew Rahtz"
                            },
                            "reply_to_user": {
                                "account_id": 159364,
                                "reputation": 674,
                                "user_id": 378647,
                                "user_type": "registered",
                                "accept_rate": 67,
                                "display_name": "Andres Felipe"
                            },
                            "edited": false,
                            "score": 1,
                            "creation_date": 1519981026,
                            "post_id": 43285333,
                            "comment_id": 85136400,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/43285333#comment85136400_43285333",
                            "body": "@gantzer89 That&#39;s right. If you load a pretrained network, the checkpoint will include the values for the mean and variance calculated during training. The mean and variance shouldn&#39;t be updated during testing."
                        },
                        {
                            "owner": {
                                "account_id": 4148701,
                                "reputation": 15803,
                                "user_id": 3924118,
                                "user_type": "registered",
                                "accept_rate": 35,
                                "display_name": "nbro"
                            },
                            "edited": false,
                            "score": 0,
                            "creation_date": 1531061394,
                            "post_id": 43285333,
                            "comment_id": 89445347,
                            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/43285333#comment89445347_43285333",
                            "body": "Would it be possible to have a <a href=\"https://stackoverflow.com/help/mcve\">minimal, complete, and verifiable example</a> of the usage of this API? I think it would be very useful."
                        }
                    ],
                    "owner": {
                        "account_id": 10635317,
                        "reputation": 1133,
                        "user_id": 7832197,
                        "user_type": "registered",
                        "display_name": "Matthew Rahtz"
                    },
                    "comment_count": 7,
                    "is_accepted": false,
                    "score": 55,
                    "last_activity_date": 1531060608,
                    "last_edit_date": 1531060608,
                    "creation_date": 1491591617,
                    "answer_id": 43285333,
                    "question_id": 33949786,
                    "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/43285333#43285333",
                    "body": "<p>As of TensorFlow 1.0 (February 2017) there's also the high-level <a href=\"https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\" rel=\"noreferrer\" title=\"tf.layers.batch_normalization\"><code>tf.layers.batch_normalization</code></a> API included in TensorFlow itself.</p>\n\n<p>It's super simple to use:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Set this to True for training and False for testing\ntraining = tf.placeholder(tf.bool)\n\nx = tf.layers.dense(input_x, units=100)\nx = tf.layers.batch_normalization(x, training=training)\nx = tf.nn.relu(x)\n</code></pre>\n\n<p>...except that it adds extra ops to the graph (for updating its mean and variance variables) in such a way that they won't be dependencies of your training op. You can either just run the ops separately:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nsess.run([train_op, extra_update_ops], ...)\n</code></pre>\n\n<p>or add the update ops as dependencies of your training op manually, then just run your training op as normal:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(extra_update_ops):\n    train_op = optimizer.minimize(loss)\n...\nsess.run([train_op], ...)\n</code></pre>\n"
                }
            ],
            "owner": {
                "account_id": 3714312,
                "reputation": 1003,
                "user_id": 3090897,
                "user_type": "registered",
                "display_name": "Shawn Lee"
            },
            "comment_count": 4,
            "is_answered": true,
            "accepted_answer_id": 33950177,
            "answer_count": 8,
            "score": 79,
            "last_activity_date": 1531330479,
            "creation_date": 1448594272,
            "last_edit_date": 1531060944,
            "question_id": 33949786,
            "link": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow",
            "title": "How could I use batch normalization in TensorFlow?",
            "body": "<p>I would like to use <em>batch normalization</em> in TensorFlow. I found the related C++ source code in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc\" rel=\"noreferrer\"><code>core/ops/nn_ops.cc</code></a>. However, I did not find it documented on tensorflow.org.</p>\n\n<p>BN has different semantics in MLP and CNN, so I am not sure what exactly this BN does.</p>\n\n<p>I did not find a method called <code>MovingMoments</code> either.</p>\n"
        }
    ],
    "has_more": false,
    "quota_max": 10000,
    "quota_remaining": 6881
}