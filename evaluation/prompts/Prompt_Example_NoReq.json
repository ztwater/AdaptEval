[
    {
        "namespace": "10.utils.add_logging_level",
        "prompt": [
            "I will provide you with a code snippet to reuse. You should adapt the snippet into a target code base according to its intra-file context. \n\n### Reused Snippet:\n```python\ndef addLoggingLevel(levelName, levelNum, methodName=None):\n    \"\"\"\n    Comprehensively adds a new logging level to the `logging` module and the\n    currently configured logging class.\n\n    `levelName` becomes an attribute of the `logging` module with the value\n    `levelNum`. `methodName` becomes a convenience method for both `logging`\n    itself and the class returned by `logging.getLoggerClass()` (usually just\n    `logging.Logger`). If `methodName` is not specified, `levelName.lower()` is\n    used.\n\n    To avoid accidental clobberings of existing attributes, this method will\n    raise an `AttributeError` if the level name is already an attribute of the\n    `logging` module or if the method name is already present \n\n    Example\n    -------\n    >>> addLoggingLevel('TRACE', logging.DEBUG - 5)\n    >>> logging.getLogger(__name__).setLevel(\"TRACE\")\n    >>> logging.getLogger(__name__).trace('that worked')\n    >>> logging.trace('so did this')\n    >>> logging.TRACE\n    5\n\n    \"\"\"\n    if not methodName:\n        methodName = levelName.lower()\n\n    if hasattr(logging, levelName):\n       raise AttributeError('{} already defined in logging module'.format(levelName))\n    if hasattr(logging, methodName):\n       raise AttributeError('{} already defined in logging module'.format(methodName))\n    if hasattr(logging.getLoggerClass(), methodName):\n       raise AttributeError('{} already defined in logger class'.format(methodName))\n\n    # This method was inspired by the answers to Stack Overflow post\n    # http://stackoverflow.com/q/2183233/2988730, especially\n    # http://stackoverflow.com/a/13638084/2988730\n    def logForLevel(self, message, *args, **kwargs):\n        if self.isEnabledFor(levelNum):\n            self._log(levelNum, message, args, **kwargs)\n    def logToRoot(message, *args, **kwargs):\n        logging.log(levelNum, message, *args, **kwargs)\n\n    logging.addLevelName(levelNum, levelName)\n    setattr(logging, levelName, levelNum)\n    setattr(logging.getLoggerClass(), methodName, logForLevel)\n    setattr(logging, methodName, logToRoot)\n\n```\n\n\n### In-file Context:\n```python\n\"\"\"Utility functions used throughout the code, kept here to allow re use and/or minimize clutter elsewhere.\"\"\"\n\nimport asyncio\nimport logging\nimport re\nfrom datetime import datetime\nfrom typing import List, Optional\n\nfrom apprise import NotifyType\nfrom async_lru import alru_cache\nfrom pyunifiprotect import ProtectApiClient\nfrom pyunifiprotect.data.nvr import Event\n\nfrom unifi_protect_backup import notifications\n\nlogger = logging.getLogger(__name__)\n\n# insert your adapted method here.\ncolor_logging = False\n\n\ndef add_color_to_record_levelname(record):\n    \"\"\"Colorizes logging level names.\"\"\"\n    levelno = record.levelno\n    if levelno >= logging.CRITICAL:\n        color = '\\x1b[31;1m'  # RED\n    elif levelno >= logging.ERROR:\n        color = '\\x1b[31;1m'  # RED\n    elif levelno >= logging.WARNING:\n        color = '\\x1b[33;1m'  # YELLOW\n    elif levelno >= logging.INFO:\n        color = '\\x1b[32;1m'  # GREEN\n    elif levelno >= logging.DEBUG:\n        color = '\\x1b[36;1m'  # CYAN\n    elif levelno >= logging.EXTRA_DEBUG:\n        color = '\\x1b[35;1m'  # MAGENTA\n    else:\n        color = '\\x1b[0m'\n\n    return f\"{color}{record.levelname}\\x1b[0m\"\n\n\nclass AppriseStreamHandler(logging.StreamHandler):\n    \"\"\"Logging handler that also sends logging output to configured Apprise notifiers.\"\"\"\n\n    def __init__(self, color_logging: bool, *args, **kwargs):\n        \"\"\"Init.\n\n        Args:\n            color_logging (bool): If true logging levels will be colorized\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.color_logging = color_logging\n\n    def _emit_apprise(self, record):\n        try:\n            loop = asyncio.get_event_loop()\n        except RuntimeError:\n            return  # There is no running loop\n\n        msg = self.format(record)\n        logging_map = {\n            logging.ERROR: NotifyType.FAILURE,\n            logging.WARNING: NotifyType.WARNING,\n            logging.INFO: NotifyType.INFO,\n            logging.DEBUG: NotifyType.INFO,\n            logging.EXTRA_DEBUG: NotifyType.INFO,\n            logging.WEBSOCKET_DATA: NotifyType.INFO,\n        }\n\n        # Only try notifying if there are notification servers configured\n        # and the asyncio loop isn't closed (aka we are quitting)\n        if notifications.notifier.servers and not loop.is_closed():\n            notify = notifications.notifier.async_notify(\n                body=msg,\n                title=record.levelname,\n                notify_type=logging_map[record.levelno],\n                tag=[record.levelname],\n            )\n            if loop.is_running():\n                asyncio.create_task(notify)\n            else:\n                loop.run_until_complete(notify)\n\n    def _emit_stream(self, record):\n        record.levelname = f\"{record.levelname:^11s}\"  # Pad level name to max width\n        if self.color_logging:\n            record.levelname = add_color_to_record_levelname(record)\n\n        msg = self.format(record)\n        stream = self.stream\n        # issue 35046: merged two stream.writes into one.\n        stream.write(msg + self.terminator)\n        self.flush()\n\n    def emit(self, record):\n        \"\"\"Emit log to stdout and apprise.\"\"\"\n        try:\n            self._emit_apprise(record)\n        except RecursionError:  # See issue 36272\n            raise\n        except Exception:\n            self.handleError(record)\n\n        try:\n            self._emit_stream(record)\n        except RecursionError:  # See issue 36272\n            raise\n        except Exception:\n            self.handleError(record)\n\n\ndef create_logging_handler(format, color_logging):\n    \"\"\"Constructs apprise logging handler for the given format.\"\"\"\n    date_format = \"%Y-%m-%d %H:%M:%S\"\n    style = '{'\n\n    sh = AppriseStreamHandler(color_logging)\n    formatter = logging.Formatter(format, date_format, style)\n    sh.setFormatter(formatter)\n    return sh\n\n\ndef setup_logging(verbosity: int, color_logging: bool = False, apprise_notifiers: List[str] = []) -> None:\n    \"\"\"Configures loggers to provided the desired level of verbosity.\n\n    Verbosity 0: Only log info messages created by `unifi-protect-backup`, and all warnings\n    verbosity 1: Only log info & debug messages created by `unifi-protect-backup`, and all warnings\n    verbosity 2: Log info & debug messages created by `unifi-protect-backup`, command output, and\n                 all warnings\n    Verbosity 3: Log debug messages created by `unifi-protect-backup`, command output, all info\n                 messages, and all warnings\n    Verbosity 4: Log debug messages created by `unifi-protect-backup` command output, all info\n                 messages, all warnings, and websocket data\n    Verbosity 5: Log websocket data, command output, all debug messages, all info messages and all\n                 warnings\n\n    Args:\n        verbosity (int): The desired level of verbosity\n        color_logging (bool): If colors should be used in the log (default=False)\n        apprise_notifiers (List[str]): Notification services to hook into the logger\n\n    \"\"\"\n    add_logging_level(\n        'EXTRA_DEBUG',\n        logging.DEBUG - 1,\n    )\n    add_logging_level(\n        'WEBSOCKET_DATA',\n        logging.DEBUG - 2,\n    )\n\n    format = \"{asctime} [{levelname:^11s}] {name:<42} :  {message}\"\n    sh = create_logging_handler(format, color_logging)\n\n    logger = logging.getLogger(\"unifi_protect_backup\")\n    logger.addHandler(sh)\n    logger.propagate = False\n\n    if verbosity == 0:\n        logging.basicConfig(level=logging.WARN, handlers=[sh])\n        logger.setLevel(logging.INFO)\n    elif verbosity == 1:\n        logging.basicConfig(level=logging.WARN, handlers=[sh])\n        logger.setLevel(logging.DEBUG)\n    elif verbosity == 2:\n        logging.basicConfig(level=logging.WARN, handlers=[sh])\n        logger.setLevel(logging.EXTRA_DEBUG)  # type: ignore\n    elif verbosity == 3:\n        logging.basicConfig(level=logging.INFO, handlers=[sh])\n        logger.setLevel(logging.EXTRA_DEBUG)  # type: ignore\n    elif verbosity == 4:\n        logging.basicConfig(level=logging.INFO, handlers=[sh])\n        logger.setLevel(logging.WEBSOCKET_DATA)  # type: ignore\n    elif verbosity >= 5:\n        logging.basicConfig(level=logging.DEBUG, handlers=[sh])\n        logger.setLevel(logging.WEBSOCKET_DATA)  # type: ignore\n\n\ndef setup_event_logger(logger, color_logging):\n    \"\"\"Sets up a logger that also displays the event ID currently being processed.\"\"\"\n    format = \"{asctime} [{levelname:^11s}] {name:<42} :{event}  {message}\"\n    sh = create_logging_handler(format, color_logging)\n    logger.addHandler(sh)\n    logger.propagate = False\n\n\n_suffixes = [\"B\", \"KiB\", \"MiB\", \"GiB\", \"TiB\", \"PiB\", \"EiB\", \"ZiB\", \"YiB\"]\n\n\ndef human_readable_size(num: float):\n    \"\"\"Turns a number into a human readable number with ISO/IEC 80000 binary prefixes.\n\n    Based on: https://stackoverflow.com/a/1094933\n\n    Args:\n        num (int): The number to be converted into human readable format\n    \"\"\"\n    for unit in _suffixes:\n        if abs(num) < 1024.0:\n            return f\"{num:3.1f}{unit}\"\n        num /= 1024.0\n    raise ValueError(\"`num` too large, ran out of prefixes\")\n\n\ndef human_readable_to_float(num: str):\n    \"\"\"Turns a human readable ISO/IEC 80000 suffix value to its full float value.\"\"\"\n    pattern = r\"([\\d.]+)(\" + \"|\".join(_suffixes) + \")\"\n    result = re.match(pattern, num)\n    if result is None:\n        raise ValueError(f\"Value '{num}' is not a valid ISO/IEC 80000 binary value\")\n\n    value = float(result[1])\n    suffix = result[2]\n    multiplier = 1024 ** _suffixes.index(suffix)\n    return value * multiplier\n\n\n# Cached so that actions like uploads can continue when the connection to the api is lost\n# No max size, and a 6 hour ttl\n@alru_cache(None, ttl=60 * 60 * 6)\nasync def get_camera_name(protect: ProtectApiClient, id: str):\n    \"\"\"Returns the name for the camera with the given ID.\n\n    If the camera ID is not know, it tries refreshing the cached data\n    \"\"\"\n    # Wait for unifi protect to be connected\n    await protect.connect_event.wait()  # type: ignore\n\n    try:\n        return protect.bootstrap.cameras[id].name\n    except KeyError:\n        # Refresh cameras\n        logger.debug(f\"Unknown camera id: '{id}', checking API\")\n\n        await protect.update(force=True)\n\n        try:\n            name = protect.bootstrap.cameras[id].name\n        except KeyError:\n            logger.debug(f\"Unknown camera id: '{id}'\")\n            raise\n\n        logger.debug(f\"Found camera - {id}: {name}\")\n        return name\n\n\nclass SubprocessException(Exception):\n    \"\"\"Class to capture: stdout, stderr, and return code of Subprocess errors.\"\"\"\n\n    def __init__(self, stdout, stderr, returncode):\n        \"\"\"Exception class for when rclone does not exit with `0`.\n\n        Args:\n          stdout (str): What rclone output to stdout\n          stderr (str): What rclone output to stderr\n          returncode (str): The return code of the rclone process\n        \"\"\"\n        super().__init__()\n        self.stdout: str = stdout\n        self.stderr: str = stderr\n        self.returncode: int = returncode\n\n    def __str__(self):\n        \"\"\"Turns exception into a human readable form.\"\"\"\n        return f\"Return Code: {self.returncode}\\nStdout:\\n{self.stdout}\\nStderr:\\n{self.stderr}\"\n\n\nasync def run_command(cmd: str, data=None):\n    \"\"\"Runs the given command returning the exit code, stdout and stderr.\"\"\"\n    proc = await asyncio.create_subprocess_shell(\n        cmd,\n        stdin=asyncio.subprocess.PIPE,\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE,\n    )\n    stdout, stderr = await proc.communicate(data)\n    stdout = stdout.decode()\n    stdout_indented = '\\t' + stdout.replace('\\n', '\\n\\t').strip()\n    stderr = stderr.decode()\n    stderr_indented = '\\t' + stderr.replace('\\n', '\\n\\t').strip()\n\n    if proc.returncode != 0:\n        logger.error(f\"Failed to run: '{cmd}\")\n        logger.error(f\"stdout:\\n{stdout_indented}\")\n        logger.error(f\"stderr:\\n{stderr_indented}\")\n    else:\n        logger.extra_debug(f\"stdout:\\n{stdout_indented}\")  # type: ignore\n        logger.extra_debug(f\"stderr:\\n{stderr_indented}\")  # type: ignore\n\n    return proc.returncode, stdout, stderr\n\n\nclass VideoQueue(asyncio.Queue):\n    \"\"\"A queue that limits the number of bytes it can store rather than discrete entries.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Init.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._bytes_sum = 0\n\n    def qsize(self):\n        \"\"\"Number of items in the queue.\"\"\"\n        return self._bytes_sum\n\n    def qsize_files(self):\n        \"\"\"Number of items in the queue.\"\"\"\n        return super().qsize()\n\n    def _get(self):\n        data = self._queue.popleft()\n        self._bytes_sum -= len(data[1])\n        return data\n\n    def _put(self, item: tuple[Event, bytes]):\n        self._queue.append(item)  # type: ignore\n        self._bytes_sum += len(item[1])\n\n    def full(self, item: tuple[Event, bytes] = None):\n        \"\"\"Return True if there are maxsize bytes in the queue.\n\n        optionally if `item` is provided, it will return False if there is enough space to\n        fit it, otherwise it will return True\n\n        Note: if the Queue was initialized with maxsize=0 (the default),\n        then full() is never True.\n        \"\"\"\n        if self._maxsize <= 0:  # type: ignore\n            return False\n        else:\n            if item is None:\n                return self.qsize() >= self._maxsize  # type: ignore\n            else:\n                return self.qsize() + len(item[1]) >= self._maxsize  # type: ignore\n\n    async def put(self, item: tuple[Event, bytes]):\n        \"\"\"Put an item into the queue.\n\n        Put an item into the queue. If the queue is full, wait until a free\n        slot is available before adding item.\n        \"\"\"\n        if len(item[1]) > self._maxsize:  # type: ignore\n            raise ValueError(\n                f\"Item is larger ({human_readable_size(len(item[1]))}) \"\n                f\"than the size of the buffer ({human_readable_size(self._maxsize)})\"  # type: ignore\n            )\n\n        while self.full(item):\n            putter = self._loop.create_future()  # type: ignore\n            self._putters.append(putter)  # type: ignore\n            try:\n                await putter\n            except:  # noqa: E722\n                putter.cancel()  # Just in case putter is not done yet.\n                try:\n                    # Clean self._putters from canceled putters.\n                    self._putters.remove(putter)  # type: ignore\n                except ValueError:\n                    # The putter could be removed from self._putters by a\n                    # previous get_nowait call.\n                    pass\n                if not self.full(item) and not putter.cancelled():\n                    # We were woken up by get_nowait(), but can't take\n                    # the call.  Wake up the next in line.\n                    self._wakeup_next(self._putters)  # type: ignore\n                raise\n        return self.put_nowait(item)\n\n    def put_nowait(self, item: tuple[Event, bytes]):\n        \"\"\"Put an item into the queue without blocking.\n\n        If no free slot is immediately available, raise QueueFull.\n        \"\"\"\n        if self.full(item):\n            raise asyncio.QueueFull\n        self._put(item)\n        self._unfinished_tasks += 1  # type: ignore\n        self._finished.clear()  # type: ignore\n        self._wakeup_next(self._getters)  # type: ignore\n\n\nasync def wait_until(dt):\n    \"\"\"Sleep until the specified datetime.\"\"\"\n    now = datetime.now()\n    await asyncio.sleep((dt - now).total_seconds())\n\n```\n\n\nPlease write out function after adaptation in the following section:\n### Adapted Function:"
        ]
    }
]
